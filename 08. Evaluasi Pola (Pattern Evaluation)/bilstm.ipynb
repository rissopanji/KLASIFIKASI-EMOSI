{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>conversation_id_str</th>\n",
       "      <th>created_at</th>\n",
       "      <th>favorite_count</th>\n",
       "      <th>full_text</th>\n",
       "      <th>id_str</th>\n",
       "      <th>image_url</th>\n",
       "      <th>in_reply_to_screen_name</th>\n",
       "      <th>lang</th>\n",
       "      <th>location</th>\n",
       "      <th>quote_count</th>\n",
       "      <th>reply_count</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>tweet_url</th>\n",
       "      <th>user_id_str</th>\n",
       "      <th>username</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1807201203832644018</td>\n",
       "      <td>Sat Jun 29 23:55:17 +0000 2024</td>\n",
       "      <td>0</td>\n",
       "      <td>@infomalang selamat pagi min minta tolong info...</td>\n",
       "      <td>1807201203832644018</td>\n",
       "      <td>NaN</td>\n",
       "      <td>infomalang</td>\n",
       "      <td>in</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>https://x.com/AzahraDean19363/status/180720120...</td>\n",
       "      <td>1710774704037089280</td>\n",
       "      <td>AzahraDean19363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1807040145256034783</td>\n",
       "      <td>Sat Jun 29 23:37:54 +0000 2024</td>\n",
       "      <td>0</td>\n",
       "      <td>@danu_kata @kaveits First impression utk yg ba...</td>\n",
       "      <td>1807196829161042011</td>\n",
       "      <td>NaN</td>\n",
       "      <td>danu_kata</td>\n",
       "      <td>in</td>\n",
       "      <td>Timbuktu</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>https://x.com/apiekluvdan/status/1807196829161...</td>\n",
       "      <td>1473134795341266950</td>\n",
       "      <td>apiekluvdan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1807192797939953929</td>\n",
       "      <td>Sat Jun 29 23:21:53 +0000 2024</td>\n",
       "      <td>1</td>\n",
       "      <td>Kursus Pendidikan Agama https://t.co/0YTwfKFUf0</td>\n",
       "      <td>1807192797939953929</td>\n",
       "      <td>https://pbs.twimg.com/media/GRRwRbIaEAEZiVm.jpg</td>\n",
       "      <td>NaN</td>\n",
       "      <td>in</td>\n",
       "      <td>Kuningan, Indonesia</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>https://x.com/ataulalaa/status/180719279793995...</td>\n",
       "      <td>742419990</td>\n",
       "      <td>ataulalaa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1806952429512233041</td>\n",
       "      <td>Sat Jun 29 22:56:05 +0000 2024</td>\n",
       "      <td>1</td>\n",
       "      <td>@imminenotyours_ @innovacommunity Biasanya bel...</td>\n",
       "      <td>1807186304561758344</td>\n",
       "      <td>NaN</td>\n",
       "      <td>imminenotyours_</td>\n",
       "      <td>in</td>\n",
       "      <td>Colorado, USA</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>https://x.com/dabudabu6969/status/180718630456...</td>\n",
       "      <td>711377077916794880</td>\n",
       "      <td>dabudabu6969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1806952429512233041</td>\n",
       "      <td>Sat Jun 29 22:54:41 +0000 2024</td>\n",
       "      <td>0</td>\n",
       "      <td>@innovacommunity Paling bener ikut kursus dah</td>\n",
       "      <td>1807185953393721541</td>\n",
       "      <td>NaN</td>\n",
       "      <td>innovacommunity</td>\n",
       "      <td>in</td>\n",
       "      <td>Colorado, USA</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>https://x.com/dabudabu6969/status/180718595339...</td>\n",
       "      <td>711377077916794880</td>\n",
       "      <td>dabudabu6969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1235</th>\n",
       "      <td>1804304762684477570</td>\n",
       "      <td>Sat Jun 22 00:05:52 +0000 2024</td>\n",
       "      <td>0</td>\n",
       "      <td>Assalamualaikum &amp;amp; Good morning Have a bles...</td>\n",
       "      <td>1804304762684477570</td>\n",
       "      <td>https://pbs.twimg.com/media/GQotnpLakAUHJm_.jpg</td>\n",
       "      <td>NaN</td>\n",
       "      <td>in</td>\n",
       "      <td>Muar, Johor</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>https://x.com/APemakanan/status/18043047626844...</td>\n",
       "      <td>1278486349599748097</td>\n",
       "      <td>APemakanan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1236</th>\n",
       "      <td>1804299666043080977</td>\n",
       "      <td>Sat Jun 22 00:01:15 +0000 2024</td>\n",
       "      <td>0</td>\n",
       "      <td>@Piyu_chann tempat kursus mah gak kenal tutup ...</td>\n",
       "      <td>1804303602141532583</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Piyu_chann</td>\n",
       "      <td>in</td>\n",
       "      <td>Jakarta Capital Region</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>https://x.com/kesurupaaan/status/1804303602141...</td>\n",
       "      <td>1577180584031371265</td>\n",
       "      <td>kesurupaaan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1237</th>\n",
       "      <td>1804116632228499520</td>\n",
       "      <td>Fri Jun 21 23:55:55 +0000 2024</td>\n",
       "      <td>0</td>\n",
       "      <td>@bturn2c aku pernah tapi kurang ngerti paket c...</td>\n",
       "      <td>1804302258194256144</td>\n",
       "      <td>NaN</td>\n",
       "      <td>bibequwu</td>\n",
       "      <td>in</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>https://x.com/ggelatou/status/1804302258194256144</td>\n",
       "      <td>1513078233574834176</td>\n",
       "      <td>ggelatou</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1238</th>\n",
       "      <td>1804148890708681061</td>\n",
       "      <td>Fri Jun 21 23:50:22 +0000 2024</td>\n",
       "      <td>1</td>\n",
       "      <td>@tanyarlfes Daripada joki mending buka kursus ...</td>\n",
       "      <td>1804300861398749310</td>\n",
       "      <td>NaN</td>\n",
       "      <td>tanyarlfes</td>\n",
       "      <td>in</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>https://x.com/kecoabunting/status/180430086139...</td>\n",
       "      <td>611902187</td>\n",
       "      <td>kecoabunting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1239</th>\n",
       "      <td>1804294931500105759</td>\n",
       "      <td>Fri Jun 21 23:26:48 +0000 2024</td>\n",
       "      <td>0</td>\n",
       "      <td>Tu lah dia kursus kahwin lebai low class duk c...</td>\n",
       "      <td>1804294931500105759</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>in</td>\n",
       "      <td>Selangor, Malaysia</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>https://x.com/zaphree/status/1804294931500105759</td>\n",
       "      <td>233585762</td>\n",
       "      <td>zaphree</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1240 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      conversation_id_str                      created_at  favorite_count  \\\n",
       "0     1807201203832644018  Sat Jun 29 23:55:17 +0000 2024               0   \n",
       "1     1807040145256034783  Sat Jun 29 23:37:54 +0000 2024               0   \n",
       "2     1807192797939953929  Sat Jun 29 23:21:53 +0000 2024               1   \n",
       "3     1806952429512233041  Sat Jun 29 22:56:05 +0000 2024               1   \n",
       "4     1806952429512233041  Sat Jun 29 22:54:41 +0000 2024               0   \n",
       "...                   ...                             ...             ...   \n",
       "1235  1804304762684477570  Sat Jun 22 00:05:52 +0000 2024               0   \n",
       "1236  1804299666043080977  Sat Jun 22 00:01:15 +0000 2024               0   \n",
       "1237  1804116632228499520  Fri Jun 21 23:55:55 +0000 2024               0   \n",
       "1238  1804148890708681061  Fri Jun 21 23:50:22 +0000 2024               1   \n",
       "1239  1804294931500105759  Fri Jun 21 23:26:48 +0000 2024               0   \n",
       "\n",
       "                                              full_text               id_str  \\\n",
       "0     @infomalang selamat pagi min minta tolong info...  1807201203832644018   \n",
       "1     @danu_kata @kaveits First impression utk yg ba...  1807196829161042011   \n",
       "2       Kursus Pendidikan Agama https://t.co/0YTwfKFUf0  1807192797939953929   \n",
       "3     @imminenotyours_ @innovacommunity Biasanya bel...  1807186304561758344   \n",
       "4         @innovacommunity Paling bener ikut kursus dah  1807185953393721541   \n",
       "...                                                 ...                  ...   \n",
       "1235  Assalamualaikum &amp; Good morning Have a bles...  1804304762684477570   \n",
       "1236  @Piyu_chann tempat kursus mah gak kenal tutup ...  1804303602141532583   \n",
       "1237  @bturn2c aku pernah tapi kurang ngerti paket c...  1804302258194256144   \n",
       "1238  @tanyarlfes Daripada joki mending buka kursus ...  1804300861398749310   \n",
       "1239  Tu lah dia kursus kahwin lebai low class duk c...  1804294931500105759   \n",
       "\n",
       "                                            image_url in_reply_to_screen_name  \\\n",
       "0                                                 NaN              infomalang   \n",
       "1                                                 NaN               danu_kata   \n",
       "2     https://pbs.twimg.com/media/GRRwRbIaEAEZiVm.jpg                     NaN   \n",
       "3                                                 NaN         imminenotyours_   \n",
       "4                                                 NaN         innovacommunity   \n",
       "...                                               ...                     ...   \n",
       "1235  https://pbs.twimg.com/media/GQotnpLakAUHJm_.jpg                     NaN   \n",
       "1236                                              NaN              Piyu_chann   \n",
       "1237                                              NaN                bibequwu   \n",
       "1238                                              NaN              tanyarlfes   \n",
       "1239                                              NaN                     NaN   \n",
       "\n",
       "     lang                location  quote_count  reply_count  retweet_count  \\\n",
       "0      in                     NaN            0            2              0   \n",
       "1      in                Timbuktu            0            1              0   \n",
       "2      in     Kuningan, Indonesia            0            0              0   \n",
       "3      in           Colorado, USA            0            0              0   \n",
       "4      in           Colorado, USA            0            0              0   \n",
       "...   ...                     ...          ...          ...            ...   \n",
       "1235   in             Muar, Johor            0            0              0   \n",
       "1236   in  Jakarta Capital Region            0            1              0   \n",
       "1237   in                     NaN            0            0              0   \n",
       "1238   in                     NaN            0            0              0   \n",
       "1239   in      Selangor, Malaysia            0            1              0   \n",
       "\n",
       "                                              tweet_url          user_id_str  \\\n",
       "0     https://x.com/AzahraDean19363/status/180720120...  1710774704037089280   \n",
       "1     https://x.com/apiekluvdan/status/1807196829161...  1473134795341266950   \n",
       "2     https://x.com/ataulalaa/status/180719279793995...            742419990   \n",
       "3     https://x.com/dabudabu6969/status/180718630456...   711377077916794880   \n",
       "4     https://x.com/dabudabu6969/status/180718595339...   711377077916794880   \n",
       "...                                                 ...                  ...   \n",
       "1235  https://x.com/APemakanan/status/18043047626844...  1278486349599748097   \n",
       "1236  https://x.com/kesurupaaan/status/1804303602141...  1577180584031371265   \n",
       "1237  https://x.com/ggelatou/status/1804302258194256144  1513078233574834176   \n",
       "1238  https://x.com/kecoabunting/status/180430086139...            611902187   \n",
       "1239   https://x.com/zaphree/status/1804294931500105759            233585762   \n",
       "\n",
       "             username  \n",
       "0     AzahraDean19363  \n",
       "1         apiekluvdan  \n",
       "2           ataulalaa  \n",
       "3        dabudabu6969  \n",
       "4        dabudabu6969  \n",
       "...               ...  \n",
       "1235       APemakanan  \n",
       "1236      kesurupaaan  \n",
       "1237         ggelatou  \n",
       "1238     kecoabunting  \n",
       "1239          zaphree  \n",
       "\n",
       "[1240 rows x 15 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "file_path = '../08. Evaluasi Pola (Pattern Evaluation)/Dataset/kursus.csv'\n",
    "\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\TEMP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    }
   ],
   "source": [
    "# Import libraries yang diperlukan\n",
    "import string\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "import pickle\n",
    "from nltk.tokenize import word_tokenize\n",
    "from mpstemmer import MPStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Unduh stopwords dari NLTK\n",
    "nltk.download('stopwords')\n",
    "stemmer = MPStemmer()\n",
    "\n",
    "# Memuat model analisis emosi yang sudah dilatih\n",
    "model = load_model('../07. Pemodelan Data (Data Mining)/model/model-bilstm.h5')\n",
    "\n",
    "class Emotion:\n",
    "    @staticmethod\n",
    "    def classify_emotion(data):\n",
    "\n",
    "        def lower_case(text):\n",
    "            return text.lower()\n",
    "\n",
    "        def remove_tweet_special(text):\n",
    "            text = text.replace('\\\\t', \" \").replace('\\\\n', \" \").replace('\\\\u', \" \").replace('\\\\', \"\")\n",
    "            text = text.encode('ascii', 'replace').decode('ascii')\n",
    "            text = ' '.join(re.sub(r\"([@#][A-Za-z0-9]+)|(\\w+:\\/\\/\\S+)\", \" \", text).split())\n",
    "            return text.replace(\"http://\", \" \").replace(\"https://\", \" \")\n",
    "\n",
    "        def remove_number(text):\n",
    "            return re.sub(r\"\\d+\", \"\", text)\n",
    "\n",
    "        def remove_punctuation(text):\n",
    "            return text.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "\n",
    "        def remove_whitespace_LT(text):\n",
    "            return text.strip()\n",
    "\n",
    "        def remove_whitespace_multiple(text):\n",
    "            return re.sub(r'\\s+', ' ', text)\n",
    "\n",
    "        def remove_singl_char(text):\n",
    "            return re.sub(r\"\\b[a-zA-Z]\\b\", \"\", text)\n",
    "\n",
    "        def remove_repeated_char(text):\n",
    "            return re.sub(r'(.)\\1+', r'\\1', text)\n",
    "\n",
    "        def word_tokenize_wrapper(text):\n",
    "            return word_tokenize(text)\n",
    "\n",
    "        normalizad_word = pd.read_csv(\"./utils/kamus-alay.csv\")\n",
    "        normalizad_word_dict = {}\n",
    "\n",
    "        for index, row in normalizad_word.iterrows():\n",
    "            if row[0] not in normalizad_word_dict:\n",
    "                normalizad_word_dict[row[0]] = row[1]\n",
    "\n",
    "        def normalized_term(document):\n",
    "            return [normalizad_word_dict[term] if term in normalizad_word_dict else term for term in document]\n",
    "\n",
    "        def stem_wrapper(term):\n",
    "            return [stemmer.stem(word) for word in term]\n",
    "\n",
    "        stop_words = stopwords.words('indonesian')\n",
    "        stop_words = [word for word in stop_words if word not in ['tidak', 'baik', 'jelek', 'jangan', 'belum', 'bukan', \"enggak\", \"engga\", \"bener\", \"benar\"]]\n",
    "        stop_words.extend([\"yg\", \"dg\", \"rt\", \"dgn\", \"ny\", \"d\", 'klo', 'kalo', 'amp', 'biar', 'bikin', 'bilang', \n",
    "                            'gak', 'ga', 'krn', 'nya', 'nih', 'sih', 'si', 'tau', 'tdk', 'tuh', 'utk', 'ya', \n",
    "                            'jd', 'jgn', 'sdh', 'aja', 'n', 't', 'nyg', 'hehe', 'pen', 'u', 'nan', 'loh', 'rt',\n",
    "                            '&amp', 'yah'])\n",
    "\n",
    "        txt_stopword = pd.read_csv(\"utils/stopwords.txt\", names=[\"stopwords\"], header=None)\n",
    "        stop_words.extend(txt_stopword[\"stopwords\"][0].split(' '))\n",
    "        stop_words = set(stop_words)\n",
    "\n",
    "        def stopwords_removal(words):\n",
    "            return [word for word in words if word not in stop_words]\n",
    "        \n",
    "        def replace_nan_with_none(data):\n",
    "            return data.applymap(lambda x: None if pd.isna(x) else x)\n",
    "\n",
    "        df = pd.DataFrame(data)\n",
    "\n",
    "        if 'predicted_label' not in df.columns:\n",
    "            df['predicted_label'] = np.nan\n",
    "            df['probability_emotion'] = np.nan\n",
    "\n",
    "        to_process_df = df[df['predicted_label'].isna()]\n",
    "\n",
    "        if not to_process_df.empty:\n",
    "            to_process_df['processed_text'] = to_process_df['full_text'].apply(lower_case)\n",
    "            to_process_df['processed_text'] = to_process_df['processed_text'].apply(remove_tweet_special)\n",
    "            to_process_df['processed_text'] = to_process_df['processed_text'].apply(remove_number)\n",
    "            to_process_df['processed_text'] = to_process_df['processed_text'].apply(remove_punctuation)\n",
    "            to_process_df['processed_text'] = to_process_df['processed_text'].apply(remove_whitespace_LT)\n",
    "            to_process_df['processed_text'] = to_process_df['processed_text'].apply(remove_whitespace_multiple)\n",
    "            to_process_df['processed_text'] = to_process_df['processed_text'].apply(remove_singl_char)\n",
    "            to_process_df['processed_text'] = to_process_df['processed_text'].apply(remove_repeated_char)\n",
    "            to_process_df['processed_text'] = to_process_df['processed_text'].apply(word_tokenize_wrapper)\n",
    "            to_process_df['processed_text'] = to_process_df['processed_text'].apply(normalized_term)\n",
    "            to_process_df['processed_text'] = to_process_df['processed_text'].apply(stem_wrapper)\n",
    "            to_process_df['processed_text'] = to_process_df['processed_text'].apply(stopwords_removal)\n",
    "            to_process_df['processed_text'] = to_process_df['processed_text'].apply(' '.join)\n",
    "\n",
    "            print(\"Text preprocessing done!\")\n",
    "\n",
    "            with open('../07. Pemodelan Data (Data Mining)/tokenizer-emotion.pickle', 'rb') as handle:\n",
    "                tokenizer = pickle.load(handle)\n",
    "\n",
    "            sequences = tokenizer.texts_to_sequences(to_process_df['processed_text'])\n",
    "            padded_sequences = pad_sequences(sequences, maxlen=50, truncating='post', padding='post')\n",
    "\n",
    "            predictions = model.predict(padded_sequences)\n",
    "            \n",
    "            emotion_labels = ['Neutral', 'Anger', 'Joy', 'Love', 'Sad', 'Fear']\n",
    "            predicted_labels = []\n",
    "            predicted_probabilities = []\n",
    "\n",
    "            for pred in predictions:\n",
    "                max_idx = np.argmax(pred)\n",
    "                predicted_labels.append(emotion_labels[max_idx])\n",
    "                predicted_probabilities.append({emotion_labels[i]: pred[i] for i in range(len(emotion_labels))})\n",
    "\n",
    "            print(\"Prediction done!\")\n",
    "\n",
    "            to_process_df['predicted_label'] = predicted_labels\n",
    "            to_process_df['probability_emotion'] = [predicted_probabilities[i][predicted_labels[i]] for i in range(len(predicted_labels))]\n",
    "\n",
    "            df.update(to_process_df)\n",
    "\n",
    "        df = replace_nan_with_none(df)\n",
    "\n",
    "        return df\n",
    "    \n",
    "    @staticmethod\n",
    "    def calculate_emotion_percentages(data):\n",
    "        total = len(data)\n",
    "        emotion_counts = {'Neutral': 0, 'Anger': 1, 'Joy': 2, 'Love': 3, 'Sad': 4, 'Fear': 5}\n",
    "\n",
    "        for item in data:\n",
    "            emotion_counts[item['predicted_label']] += 1\n",
    "        \n",
    "        percentages = {emotion: (count / total) * 100 for emotion, count in emotion_counts.items()}\n",
    "        return percentages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\TEMP\\AppData\\Local\\Temp\\ipykernel_5644\\3662870842.py:59: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  if row[0] not in normalizad_word_dict:\n",
      "C:\\Users\\TEMP\\AppData\\Local\\Temp\\ipykernel_5644\\3662870842.py:60: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  normalizad_word_dict[row[0]] = row[1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text preprocessing done!\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step\n",
      "Prediction done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\TEMP\\AppData\\Local\\Temp\\ipykernel_5644\\3662870842.py:132: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '['Neutral' 'Sad' 'Anger' ... 'Anger' 'Neutral' 'Joy']' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  df.update(to_process_df)\n",
      "C:\\Users\\TEMP\\AppData\\Local\\Temp\\ipykernel_5644\\3662870842.py:83: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  return data.applymap(lambda x: None if pd.isna(x) else x)\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"['topic', 'topic_probability'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m hasil \u001b[38;5;241m=\u001b[39m Emotion\u001b[38;5;241m.\u001b[39mclassify_emotion(data)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Menyimpan hasil hanya full_text, prediksi dan probabilitas \u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m hasil \u001b[38;5;241m=\u001b[39m \u001b[43mhasil\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43musername\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfull_text\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtopic\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtopic_probability\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpredicted_label\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mprobability_emotion\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m      6\u001b[0m hasil\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhasil/hasil_prediksi-diet(bilstm).csv\u001b[39m\u001b[38;5;124m'\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\TEMP\\miniconda3\\envs\\skripsi\\Lib\\site-packages\\pandas\\core\\frame.py:4108\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   4106\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[0;32m   4107\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[1;32m-> 4108\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_indexer_strict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcolumns\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m   4110\u001b[0m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[0;32m   4111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\TEMP\\miniconda3\\envs\\skripsi\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:6200\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[1;34m(self, key, axis_name)\u001b[0m\n\u001b[0;32m   6197\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   6198\u001b[0m     keyarr, indexer, new_indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[1;32m-> 6200\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raise_if_missing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   6202\u001b[0m keyarr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtake(indexer)\n\u001b[0;32m   6203\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[0;32m   6204\u001b[0m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\TEMP\\miniconda3\\envs\\skripsi\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:6252\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[1;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[0;32m   6249\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   6251\u001b[0m not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[38;5;241m.\u001b[39mnonzero()[\u001b[38;5;241m0\u001b[39m]]\u001b[38;5;241m.\u001b[39munique())\n\u001b[1;32m-> 6252\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mKeyError\u001b[0m: \"['topic', 'topic_probability'] not in index\""
     ]
    }
   ],
   "source": [
    "# Mengklasifikasikan emosi\n",
    "hasil = Emotion.classify_emotion(data)\n",
    "\n",
    "# Menyimpan hasil hanya full_text, prediksi dan probabilitas \n",
    "hasil = hasil[['username','full_text','topic','topic_probability','predicted_label', 'probability_emotion']]\n",
    "hasil.to_csv('hasil/hasil_prediksi-diet(bilstm).csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "skripsi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
