{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>conversation_id_str</th>\n",
       "      <th>created_at</th>\n",
       "      <th>favorite_count</th>\n",
       "      <th>full_text</th>\n",
       "      <th>id_str</th>\n",
       "      <th>image_url</th>\n",
       "      <th>in_reply_to_screen_name</th>\n",
       "      <th>lang</th>\n",
       "      <th>location</th>\n",
       "      <th>quote_count</th>\n",
       "      <th>reply_count</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>tweet_url</th>\n",
       "      <th>user_id_str</th>\n",
       "      <th>username</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1801647536496857484</td>\n",
       "      <td>Fri Jun 14 21:55:26 +0000 2024</td>\n",
       "      <td>29</td>\n",
       "      <td>@Indostransfer Asing Madura musim ini kemungki...</td>\n",
       "      <td>1801735224654762024</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Indostransfer</td>\n",
       "      <td>in</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>https://x.com/wayuuu18/status/1801735224654762024</td>\n",
       "      <td>1387311427958886400</td>\n",
       "      <td>wayuuu18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1801647536496857484</td>\n",
       "      <td>Fri Jun 14 21:52:02 +0000 2024</td>\n",
       "      <td>0</td>\n",
       "      <td>@Indostransfer Sebelumnya *cmiiw jika Persib l...</td>\n",
       "      <td>1801734368849002701</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Indostransfer</td>\n",
       "      <td>in</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>https://x.com/wayuuu18/status/1801734368849002701</td>\n",
       "      <td>1387311427958886400</td>\n",
       "      <td>wayuuu18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1801596984186302759</td>\n",
       "      <td>Fri Jun 14 21:50:38 +0000 2024</td>\n",
       "      <td>0</td>\n",
       "      <td>@FootyRankings Jeonbuk Port Lee man Persib</td>\n",
       "      <td>1801734016007344428</td>\n",
       "      <td>NaN</td>\n",
       "      <td>FootyRankings</td>\n",
       "      <td>in</td>\n",
       "      <td>Bandung</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>https://x.com/vrijeman___/status/1801734016007...</td>\n",
       "      <td>1180727063264686080</td>\n",
       "      <td>vrijeman___</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1801647536496857484</td>\n",
       "      <td>Fri Jun 14 21:48:47 +0000 2024</td>\n",
       "      <td>0</td>\n",
       "      <td>@Indostransfer @vikyfauzy Disini yg main bang ...</td>\n",
       "      <td>1801733549768511897</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Indostransfer</td>\n",
       "      <td>in</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>https://x.com/hayesstyls/status/18017335497685...</td>\n",
       "      <td>1799981034425208832</td>\n",
       "      <td>hayesstyls</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1801663965564489825</td>\n",
       "      <td>Fri Jun 14 21:43:06 +0000 2024</td>\n",
       "      <td>0</td>\n",
       "      <td>@swilkinsonbc Anjir naon warna biruuu persib m...</td>\n",
       "      <td>1801732118323605980</td>\n",
       "      <td>NaN</td>\n",
       "      <td>swilkinsonbc</td>\n",
       "      <td>in</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>https://x.com/Labeaufleur/status/1801732118323...</td>\n",
       "      <td>1544115184343531520</td>\n",
       "      <td>Labeaufleur</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5054</th>\n",
       "      <td>1798896044400595095</td>\n",
       "      <td>Fri Jun 07 01:53:33 +0000 2024</td>\n",
       "      <td>0</td>\n",
       "      <td>https://t.co/xBDsqdmcEI</td>\n",
       "      <td>1798896044400595095</td>\n",
       "      <td>https://pbs.twimg.com/media/GPb2a2nbYAA3Bcs.jpg</td>\n",
       "      <td>NaN</td>\n",
       "      <td>zxx</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>https://x.com/anindyasinjai/status/17988960444...</td>\n",
       "      <td>2258214942</td>\n",
       "      <td>anindyasinjai</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5055</th>\n",
       "      <td>1798896014994419875</td>\n",
       "      <td>Fri Jun 07 01:53:26 +0000 2024</td>\n",
       "      <td>0</td>\n",
       "      <td>https://t.co/6AK5lTgWMG</td>\n",
       "      <td>1798896014994419875</td>\n",
       "      <td>https://pbs.twimg.com/media/GPb2ZHUaEAAdogy.jpg</td>\n",
       "      <td>NaN</td>\n",
       "      <td>zxx</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>https://x.com/anindyasinjai/status/17988960149...</td>\n",
       "      <td>2258214942</td>\n",
       "      <td>anindyasinjai</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5056</th>\n",
       "      <td>1798895997164351928</td>\n",
       "      <td>Fri Jun 07 01:53:22 +0000 2024</td>\n",
       "      <td>0</td>\n",
       "      <td>https://t.co/Bewddzz4Af</td>\n",
       "      <td>1798895997164351928</td>\n",
       "      <td>https://pbs.twimg.com/media/GPb2YCHaQAADJw1.jpg</td>\n",
       "      <td>NaN</td>\n",
       "      <td>zxx</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>https://x.com/anindyasinjai/status/17988959971...</td>\n",
       "      <td>2258214942</td>\n",
       "      <td>anindyasinjai</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5057</th>\n",
       "      <td>1798895948934082931</td>\n",
       "      <td>Fri Jun 07 01:53:10 +0000 2024</td>\n",
       "      <td>0</td>\n",
       "      <td>https://t.co/5WfO1dUzbw</td>\n",
       "      <td>1798895948934082931</td>\n",
       "      <td>https://pbs.twimg.com/media/GPb2VLvaoAAKs-X.jpg</td>\n",
       "      <td>NaN</td>\n",
       "      <td>zxx</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>https://x.com/anindyasinjai/status/17988959489...</td>\n",
       "      <td>2258214942</td>\n",
       "      <td>anindyasinjai</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5058</th>\n",
       "      <td>1798895926108692777</td>\n",
       "      <td>Fri Jun 07 01:53:05 +0000 2024</td>\n",
       "      <td>0</td>\n",
       "      <td>https://t.co/KexinQzY6b</td>\n",
       "      <td>1798895926108692777</td>\n",
       "      <td>https://pbs.twimg.com/media/GPb2T-5bQAATYL0.jpg</td>\n",
       "      <td>NaN</td>\n",
       "      <td>zxx</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>https://x.com/anindyasinjai/status/17988959261...</td>\n",
       "      <td>2258214942</td>\n",
       "      <td>anindyasinjai</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5059 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      conversation_id_str                      created_at  favorite_count  \\\n",
       "0     1801647536496857484  Fri Jun 14 21:55:26 +0000 2024              29   \n",
       "1     1801647536496857484  Fri Jun 14 21:52:02 +0000 2024               0   \n",
       "2     1801596984186302759  Fri Jun 14 21:50:38 +0000 2024               0   \n",
       "3     1801647536496857484  Fri Jun 14 21:48:47 +0000 2024               0   \n",
       "4     1801663965564489825  Fri Jun 14 21:43:06 +0000 2024               0   \n",
       "...                   ...                             ...             ...   \n",
       "5054  1798896044400595095  Fri Jun 07 01:53:33 +0000 2024               0   \n",
       "5055  1798896014994419875  Fri Jun 07 01:53:26 +0000 2024               0   \n",
       "5056  1798895997164351928  Fri Jun 07 01:53:22 +0000 2024               0   \n",
       "5057  1798895948934082931  Fri Jun 07 01:53:10 +0000 2024               0   \n",
       "5058  1798895926108692777  Fri Jun 07 01:53:05 +0000 2024               0   \n",
       "\n",
       "                                              full_text               id_str  \\\n",
       "0     @Indostransfer Asing Madura musim ini kemungki...  1801735224654762024   \n",
       "1     @Indostransfer Sebelumnya *cmiiw jika Persib l...  1801734368849002701   \n",
       "2            @FootyRankings Jeonbuk Port Lee man Persib  1801734016007344428   \n",
       "3     @Indostransfer @vikyfauzy Disini yg main bang ...  1801733549768511897   \n",
       "4     @swilkinsonbc Anjir naon warna biruuu persib m...  1801732118323605980   \n",
       "...                                                 ...                  ...   \n",
       "5054                            https://t.co/xBDsqdmcEI  1798896044400595095   \n",
       "5055                            https://t.co/6AK5lTgWMG  1798896014994419875   \n",
       "5056                            https://t.co/Bewddzz4Af  1798895997164351928   \n",
       "5057                            https://t.co/5WfO1dUzbw  1798895948934082931   \n",
       "5058                            https://t.co/KexinQzY6b  1798895926108692777   \n",
       "\n",
       "                                            image_url in_reply_to_screen_name  \\\n",
       "0                                                 NaN           Indostransfer   \n",
       "1                                                 NaN           Indostransfer   \n",
       "2                                                 NaN           FootyRankings   \n",
       "3                                                 NaN           Indostransfer   \n",
       "4                                                 NaN            swilkinsonbc   \n",
       "...                                               ...                     ...   \n",
       "5054  https://pbs.twimg.com/media/GPb2a2nbYAA3Bcs.jpg                     NaN   \n",
       "5055  https://pbs.twimg.com/media/GPb2ZHUaEAAdogy.jpg                     NaN   \n",
       "5056  https://pbs.twimg.com/media/GPb2YCHaQAADJw1.jpg                     NaN   \n",
       "5057  https://pbs.twimg.com/media/GPb2VLvaoAAKs-X.jpg                     NaN   \n",
       "5058  https://pbs.twimg.com/media/GPb2T-5bQAATYL0.jpg                     NaN   \n",
       "\n",
       "     lang location  quote_count  reply_count  retweet_count  \\\n",
       "0      in      NaN            0            5              0   \n",
       "1      in      NaN            0            0              0   \n",
       "2      in  Bandung            0            0              0   \n",
       "3      in      NaN            0            3              0   \n",
       "4      in      NaN            0            0              0   \n",
       "...   ...      ...          ...          ...            ...   \n",
       "5054  zxx      NaN            0            0              0   \n",
       "5055  zxx      NaN            0            0              0   \n",
       "5056  zxx      NaN            0            0              0   \n",
       "5057  zxx      NaN            0            0              0   \n",
       "5058  zxx      NaN            0            0              0   \n",
       "\n",
       "                                              tweet_url          user_id_str  \\\n",
       "0     https://x.com/wayuuu18/status/1801735224654762024  1387311427958886400   \n",
       "1     https://x.com/wayuuu18/status/1801734368849002701  1387311427958886400   \n",
       "2     https://x.com/vrijeman___/status/1801734016007...  1180727063264686080   \n",
       "3     https://x.com/hayesstyls/status/18017335497685...  1799981034425208832   \n",
       "4     https://x.com/Labeaufleur/status/1801732118323...  1544115184343531520   \n",
       "...                                                 ...                  ...   \n",
       "5054  https://x.com/anindyasinjai/status/17988960444...           2258214942   \n",
       "5055  https://x.com/anindyasinjai/status/17988960149...           2258214942   \n",
       "5056  https://x.com/anindyasinjai/status/17988959971...           2258214942   \n",
       "5057  https://x.com/anindyasinjai/status/17988959489...           2258214942   \n",
       "5058  https://x.com/anindyasinjai/status/17988959261...           2258214942   \n",
       "\n",
       "           username  \n",
       "0          wayuuu18  \n",
       "1          wayuuu18  \n",
       "2       vrijeman___  \n",
       "3        hayesstyls  \n",
       "4       Labeaufleur  \n",
       "...             ...  \n",
       "5054  anindyasinjai  \n",
       "5055  anindyasinjai  \n",
       "5056  anindyasinjai  \n",
       "5057  anindyasinjai  \n",
       "5058  anindyasinjai  \n",
       "\n",
       "[5059 rows x 15 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "file_path = '../08. Evaluasi Pola (Pattern Evaluation)/Dataset/persib.csv'\n",
    "\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "partially initialized module 'nltk' has no attribute 'data' (most likely due to a circular import)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mre\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtokenize\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m word_tokenize\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmpstemmer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MPStemmer\n",
      "File \u001b[1;32mc:\\Users\\TEMP\\miniconda3\\envs\\skripsi\\Lib\\site-packages\\nltk\\__init__.py:153\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtag\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m    152\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtokenize\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m--> 153\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtranslate\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m    154\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtree\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m    155\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msem\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\TEMP\\miniconda3\\envs\\skripsi\\Lib\\site-packages\\nltk\\translate\\__init__.py:24\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtranslate\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbleu_score\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m sentence_bleu \u001b[38;5;28;01mas\u001b[39;00m bleu\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtranslate\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mribes_score\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m sentence_ribes \u001b[38;5;28;01mas\u001b[39;00m ribes\n\u001b[1;32m---> 24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtranslate\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmeteor_score\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m meteor_score \u001b[38;5;28;01mas\u001b[39;00m meteor\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtranslate\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m alignment_error_rate\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtranslate\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstack_decoder\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m StackDecoder\n",
      "File \u001b[1;32mc:\\Users\\TEMP\\miniconda3\\envs\\skripsi\\Lib\\site-packages\\nltk\\translate\\meteor_score.py:13\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mitertools\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m chain, product\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Callable, Iterable, List, Tuple\n\u001b[1;32m---> 13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcorpus\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m WordNetCorpusReader, wordnet\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstem\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m StemmerI\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstem\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mporter\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PorterStemmer\n",
      "File \u001b[1;32mc:\\Users\\TEMP\\miniconda3\\envs\\skripsi\\Lib\\site-packages\\nltk\\corpus\\__init__.py:64\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;124;03mNLTK corpus readers.  The modules in this package provide functions\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;124;03mthat can be used to read corpus files in a variety of formats.  These\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     59\u001b[0m \n\u001b[0;32m     60\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mre\u001b[39;00m\n\u001b[1;32m---> 64\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcorpus\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mreader\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcorpus\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LazyCorpusLoader\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtokenize\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RegexpTokenizer\n",
      "File \u001b[1;32mc:\\Users\\TEMP\\miniconda3\\envs\\skripsi\\Lib\\site-packages\\nltk\\corpus\\reader\\__init__.py:57\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Natural Language Toolkit: Corpus Readers\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Copyright (C) 2001-2023 NLTK Project\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# URL: <https://www.nltk.org/>\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# For license information, see LICENSE.TXT\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;124;03mNLTK corpus readers.  The modules in this package provide functions\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;124;03mthat can be used to read corpus fileids in a variety of formats.  These\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;124;03misort:skip_file\u001b[39;00m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m---> 57\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcorpus\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mreader\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mplaintext\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcorpus\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mreader\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcorpus\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mreader\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\TEMP\\miniconda3\\envs\\skripsi\\Lib\\site-packages\\nltk\\corpus\\reader\\plaintext.py:20\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcorpus\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mreader\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtokenize\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m---> 20\u001b[0m \u001b[38;5;28;43;01mclass\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;21;43;01mPlaintextCorpusReader\u001b[39;49;00m\u001b[43m(\u001b[49m\u001b[43mCorpusReader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;250;43m    \u001b[39;49m\u001b[38;5;124;43;03m\"\"\"\u001b[39;49;00m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;124;43;03m    Reader for corpora that consist of plaintext documents.  Paragraphs\u001b[39;49;00m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;124;43;03m    are assumed to be split using blank lines.  Sentences and words can\u001b[39;49;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;124;43;03m    overriding the ``CorpusView`` class variable.\u001b[39;49;00m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;124;43;03m    \"\"\"\u001b[39;49;00m\n\u001b[0;32m     32\u001b[0m \u001b[43m    \u001b[49m\u001b[43mCorpusView\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mStreamBackedCorpusView\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\TEMP\\miniconda3\\envs\\skripsi\\Lib\\site-packages\\nltk\\corpus\\reader\\plaintext.py:42\u001b[0m, in \u001b[0;36mPlaintextCorpusReader\u001b[1;34m()\u001b[0m\n\u001b[0;32m     32\u001b[0m CorpusView \u001b[38;5;241m=\u001b[39m StreamBackedCorpusView\n\u001b[0;32m     33\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"The corpus view class used by this reader.  Subclasses of\u001b[39;00m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;124;03m   ``PlaintextCorpusReader`` may specify alternative corpus view\u001b[39;00m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;124;03m   classes (e.g., to skip the preface sections of documents.)\"\"\"\u001b[39;00m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[0;32m     38\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m     39\u001b[0m     root,\n\u001b[0;32m     40\u001b[0m     fileids,\n\u001b[0;32m     41\u001b[0m     word_tokenizer\u001b[38;5;241m=\u001b[39mWordPunctTokenizer(),\n\u001b[1;32m---> 42\u001b[0m     sent_tokenizer\u001b[38;5;241m=\u001b[39m\u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241m.\u001b[39mLazyLoader(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtokenizers/punkt/english.pickle\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m     43\u001b[0m     para_block_reader\u001b[38;5;241m=\u001b[39mread_blankline_block,\n\u001b[0;32m     44\u001b[0m     encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf8\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     45\u001b[0m ):\n\u001b[0;32m     46\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;124;03m    Construct a new plaintext corpus reader for a set of documents\u001b[39;00m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;124;03m    located at the given root directory.  Example usage:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;124;03m        corpus into paragraph blocks.\u001b[39;00m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m     62\u001b[0m     CorpusReader\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, root, fileids, encoding)\n",
      "\u001b[1;31mAttributeError\u001b[0m: partially initialized module 'nltk' has no attribute 'data' (most likely due to a circular import)"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import string\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from mpstemmer import MPStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import pickle\n",
    "\n",
    "# Download necessary NLTK data\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Initialize stemmer\n",
    "stemmer = MPStemmer()\n",
    "\n",
    "# Load the model\n",
    "model = load_model('../07. Pemodelan Data (Data Mining)/model/model-bilstm.h5')\n",
    "\n",
    "class Emotion:\n",
    "    @staticmethod\n",
    "    def classify_emotion(data):\n",
    "        def lower_case(text):\n",
    "            return text.lower()\n",
    "\n",
    "        def remove_tweet_special(text):\n",
    "            text = text.replace('\\\\t', \" \").replace('\\\\n', \" \").replace('\\\\u', \" \").replace('\\\\', \"\")\n",
    "            text = text.encode('ascii', 'replace').decode('ascii')\n",
    "            text = ' '.join(re.sub(r\"([@#][A-Za-z0-9]+)|(\\w+:\\/\\/\\S+)\", \" \", text).split())\n",
    "            return text.replace(\"http://\", \" \").replace(\"https://\", \" \")\n",
    "\n",
    "        def remove_number(text):\n",
    "            return re.sub(r\"\\d+\", \"\", text)\n",
    "\n",
    "        def remove_punctuation(text):\n",
    "            return text.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "\n",
    "        def remove_whitespace_LT(text):\n",
    "            return text.strip()\n",
    "\n",
    "        def remove_whitespace_multiple(text):\n",
    "            return re.sub(r'\\s+', ' ', text)\n",
    "\n",
    "        def remove_singl_char(text):\n",
    "            return re.sub(r\"\\b[a-zA-Z]\\b\", \"\", text)\n",
    "\n",
    "        def remove_repeated_char(text):\n",
    "            return re.sub(r'(.)\\1+', r'\\1', text)\n",
    "\n",
    "        def word_tokenize_wrapper(text):\n",
    "            return word_tokenize(text)\n",
    "\n",
    "        normalizad_word = pd.read_csv(\"./utils/kamus-alay.csv\")\n",
    "        normalizad_word_dict = {row[0]: row[1] for index, row in normalizad_word.iterrows()}\n",
    "\n",
    "        def normalized_term(document):\n",
    "            return [normalizad_word_dict.get(term, term) for term in document]\n",
    "\n",
    "        def stem_wrapper(term):\n",
    "            return [stemmer.stem(word) for word in term]\n",
    "\n",
    "        stop_words = stopwords.words('indonesian')\n",
    "        stop_words = [word for word in stop_words if word not in ['tidak', 'baik', 'jelek', 'jangan', 'belum', 'bukan', \"enggak\", \"engga\", \"bener\", \"benar\"]]\n",
    "        stop_words.extend([\"yg\", \"dg\", \"rt\", \"dgn\", \"ny\", \"d\", 'klo', 'kalo', 'amp', 'biar', 'bikin', 'bilang', 'gak', 'ga', 'krn', 'nya', 'nih', 'sih', 'si', 'tau', 'tdk', 'tuh', 'utk', 'ya', 'jd', 'jgn', 'sdh', 'aja', 'n', 't', 'nyg', 'hehe', 'pen', 'u', 'nan', 'loh', 'rt', '&amp', 'yah'])\n",
    "\n",
    "        txt_stopword = pd.read_csv(\"utils/stopwords.txt\", names=[\"stopwords\"], header=None)\n",
    "        stop_words.extend(txt_stopword[\"stopwords\"][0].split(' '))\n",
    "        stop_words = set(stop_words)\n",
    "\n",
    "        def stopwords_removal(words):\n",
    "            return [word for word in words if word not in stop_words]\n",
    "\n",
    "        def replace_nan_with_none(data):\n",
    "            return data.applymap(lambda x: None if pd.isna(x) else x)\n",
    "\n",
    "        df = pd.DataFrame(data)\n",
    "\n",
    "        if 'predicted_label' not in df.columns:\n",
    "            df['predicted_label'] = np.nan\n",
    "            df['probability_emotion'] = np.nan\n",
    "\n",
    "        to_process_df = df[df['predicted_label'].isna()]\n",
    "\n",
    "        if not to_process_df.empty:\n",
    "            to_process_df['processed_text'] = to_process_df['full_text'].apply(lower_case)\n",
    "            to_process_df['processed_text'] = to_process_df['processed_text'].apply(remove_tweet_special)\n",
    "            to_process_df['processed_text'] = to_process_df['processed_text'].apply(remove_number)\n",
    "            to_process_df['processed_text'] = to_process_df['processed_text'].apply(remove_punctuation)\n",
    "            to_process_df['processed_text'] = to_process_df['processed_text'].apply(remove_whitespace_LT)\n",
    "            to_process_df['processed_text'] = to_process_df['processed_text'].apply(remove_whitespace_multiple)\n",
    "            to_process_df['processed_text'] = to_process_df['processed_text'].apply(remove_singl_char)\n",
    "            to_process_df['processed_text'] = to_process_df['processed_text'].apply(remove_repeated_char)\n",
    "            to_process_df['processed_text'] = to_process_df['processed_text'].apply(word_tokenize_wrapper)\n",
    "            to_process_df['processed_text'] = to_process_df['processed_text'].apply(normalized_term)\n",
    "            to_process_df['processed_text'] = to_process_df['processed_text'].apply(stem_wrapper)\n",
    "            to_process_df['processed_text'] = to_process_df['processed_text'].apply(stopwords_removal)\n",
    "            to_process_df['processed_text'] = to_process_df['processed_text'].apply(' '.join)\n",
    "\n",
    "            print(\"Text preprocessing done!\")\n",
    "\n",
    "            with open('../07. Pemodelan Data (Data Mining)/tokenizer-emotion.pickle', 'rb') as handle:\n",
    "                tokenizer = pickle.load(handle)\n",
    "\n",
    "            sequences = tokenizer.texts_to_sequences(to_process_df['processed_text'])\n",
    "            padded_sequences = pad_sequences(sequences, maxlen=50, truncating='post', padding='post')\n",
    "\n",
    "            predictions = model.predict(padded_sequences)\n",
    "            \n",
    "            emotion_labels = ['Neutral', 'Anger', 'Joy', 'Love', 'Sad', 'Fear']\n",
    "            predicted_labels = []\n",
    "            predicted_probabilities = []\n",
    "\n",
    "            for pred in predictions:\n",
    "                max_idx = np.argmax(pred)\n",
    "                predicted_labels.append(emotion_labels[max_idx])\n",
    "                predicted_probabilities.append({emotion_labels[i]: pred[i] for i in range(len(emotion_labels))})\n",
    "\n",
    "            print(\"Prediction done!\")\n",
    "\n",
    "            to_process_df['predicted_label'] = predicted_labels\n",
    "            to_process_df['probability_emotion'] = [predicted_probabilities[i][predicted_labels[i]] for i in range(len(predicted_labels))]\n",
    "\n",
    "            df.update(to_process_df)\n",
    "\n",
    "        df = replace_nan_with_none(df)\n",
    "\n",
    "        return df\n",
    "    \n",
    "    @staticmethod\n",
    "    def calculate_emotion_percentages(data):\n",
    "        total = len(data)\n",
    "        emotion_counts = {'Neutral': 0, 'Anger': 1, 'Joy': 2, 'Love': 3, 'Sad': 4, 'Fear': 5}\n",
    "\n",
    "        for item in data:\n",
    "            emotion_counts[item['predicted_label']] += 1\n",
    "        \n",
    "        percentages = {emotion: (count / total) * 100 for emotion, count in emotion_counts.items()}\n",
    "        return percentages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\TEMP\\AppData\\Local\\Temp\\ipykernel_5644\\3662870842.py:59: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  if row[0] not in normalizad_word_dict:\n",
      "C:\\Users\\TEMP\\AppData\\Local\\Temp\\ipykernel_5644\\3662870842.py:60: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  normalizad_word_dict[row[0]] = row[1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text preprocessing done!\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step\n",
      "Prediction done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\TEMP\\AppData\\Local\\Temp\\ipykernel_5644\\3662870842.py:132: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '['Neutral' 'Sad' 'Anger' ... 'Anger' 'Neutral' 'Joy']' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  df.update(to_process_df)\n",
      "C:\\Users\\TEMP\\AppData\\Local\\Temp\\ipykernel_5644\\3662870842.py:83: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  return data.applymap(lambda x: None if pd.isna(x) else x)\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"['topic', 'topic_probability'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m hasil \u001b[38;5;241m=\u001b[39m Emotion\u001b[38;5;241m.\u001b[39mclassify_emotion(data)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Menyimpan hasil hanya full_text, prediksi dan probabilitas \u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m hasil \u001b[38;5;241m=\u001b[39m \u001b[43mhasil\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43musername\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfull_text\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtopic\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtopic_probability\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpredicted_label\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mprobability_emotion\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m      6\u001b[0m hasil\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhasil/hasil_prediksi-diet(bilstm).csv\u001b[39m\u001b[38;5;124m'\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\TEMP\\miniconda3\\envs\\skripsi\\Lib\\site-packages\\pandas\\core\\frame.py:4108\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   4106\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[0;32m   4107\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[1;32m-> 4108\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_indexer_strict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcolumns\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m   4110\u001b[0m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[0;32m   4111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\TEMP\\miniconda3\\envs\\skripsi\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:6200\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[1;34m(self, key, axis_name)\u001b[0m\n\u001b[0;32m   6197\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   6198\u001b[0m     keyarr, indexer, new_indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[1;32m-> 6200\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raise_if_missing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   6202\u001b[0m keyarr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtake(indexer)\n\u001b[0;32m   6203\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[0;32m   6204\u001b[0m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\TEMP\\miniconda3\\envs\\skripsi\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:6252\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[1;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[0;32m   6249\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   6251\u001b[0m not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[38;5;241m.\u001b[39mnonzero()[\u001b[38;5;241m0\u001b[39m]]\u001b[38;5;241m.\u001b[39munique())\n\u001b[1;32m-> 6252\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mKeyError\u001b[0m: \"['topic', 'topic_probability'] not in index\""
     ]
    }
   ],
   "source": [
    "# Mengklasifikasikan emosi\n",
    "hasil = Emotion.classify_emotion(data)\n",
    "\n",
    "# Menyimpan hasil hanya full_text, prediksi dan probabilitas \n",
    "hasil = hasil[['username','full_text','topic','topic_probability','predicted_label', 'probability_emotion']]\n",
    "hasil.to_csv('hasil/hasil_prediksi-persib(bilstm).csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "skripsi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
