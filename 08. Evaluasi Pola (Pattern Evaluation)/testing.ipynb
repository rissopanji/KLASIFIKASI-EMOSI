{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>conversation_id_str</th>\n",
       "      <th>created_at</th>\n",
       "      <th>favorite_count</th>\n",
       "      <th>full_text</th>\n",
       "      <th>id_str</th>\n",
       "      <th>image_url</th>\n",
       "      <th>in_reply_to_screen_name</th>\n",
       "      <th>lang</th>\n",
       "      <th>location</th>\n",
       "      <th>quote_count</th>\n",
       "      <th>reply_count</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>tweet_url</th>\n",
       "      <th>user_id_str</th>\n",
       "      <th>username</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1801647536496857484</td>\n",
       "      <td>Fri Jun 14 21:55:26 +0000 2024</td>\n",
       "      <td>29</td>\n",
       "      <td>@Indostransfer Asing Madura musim ini kemungki...</td>\n",
       "      <td>1801735224654762024</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Indostransfer</td>\n",
       "      <td>in</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>https://x.com/wayuuu18/status/1801735224654762024</td>\n",
       "      <td>1387311427958886400</td>\n",
       "      <td>wayuuu18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1801647536496857484</td>\n",
       "      <td>Fri Jun 14 21:52:02 +0000 2024</td>\n",
       "      <td>0</td>\n",
       "      <td>@Indostransfer Sebelumnya *cmiiw jika Persib l...</td>\n",
       "      <td>1801734368849002701</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Indostransfer</td>\n",
       "      <td>in</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>https://x.com/wayuuu18/status/1801734368849002701</td>\n",
       "      <td>1387311427958886400</td>\n",
       "      <td>wayuuu18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1801596984186302759</td>\n",
       "      <td>Fri Jun 14 21:50:38 +0000 2024</td>\n",
       "      <td>0</td>\n",
       "      <td>@FootyRankings Jeonbuk Port Lee man Persib</td>\n",
       "      <td>1801734016007344428</td>\n",
       "      <td>NaN</td>\n",
       "      <td>FootyRankings</td>\n",
       "      <td>in</td>\n",
       "      <td>Bandung</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>https://x.com/vrijeman___/status/1801734016007...</td>\n",
       "      <td>1180727063264686080</td>\n",
       "      <td>vrijeman___</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1801647536496857484</td>\n",
       "      <td>Fri Jun 14 21:48:47 +0000 2024</td>\n",
       "      <td>0</td>\n",
       "      <td>@Indostransfer @vikyfauzy Disini yg main bang ...</td>\n",
       "      <td>1801733549768511897</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Indostransfer</td>\n",
       "      <td>in</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>https://x.com/hayesstyls/status/18017335497685...</td>\n",
       "      <td>1799981034425208832</td>\n",
       "      <td>hayesstyls</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1801663965564489825</td>\n",
       "      <td>Fri Jun 14 21:43:06 +0000 2024</td>\n",
       "      <td>0</td>\n",
       "      <td>@swilkinsonbc Anjir naon warna biruuu persib m...</td>\n",
       "      <td>1801732118323605980</td>\n",
       "      <td>NaN</td>\n",
       "      <td>swilkinsonbc</td>\n",
       "      <td>in</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>https://x.com/Labeaufleur/status/1801732118323...</td>\n",
       "      <td>1544115184343531520</td>\n",
       "      <td>Labeaufleur</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5054</th>\n",
       "      <td>1798896044400595095</td>\n",
       "      <td>Fri Jun 07 01:53:33 +0000 2024</td>\n",
       "      <td>0</td>\n",
       "      <td>https://t.co/xBDsqdmcEI</td>\n",
       "      <td>1798896044400595095</td>\n",
       "      <td>https://pbs.twimg.com/media/GPb2a2nbYAA3Bcs.jpg</td>\n",
       "      <td>NaN</td>\n",
       "      <td>zxx</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>https://x.com/anindyasinjai/status/17988960444...</td>\n",
       "      <td>2258214942</td>\n",
       "      <td>anindyasinjai</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5055</th>\n",
       "      <td>1798896014994419875</td>\n",
       "      <td>Fri Jun 07 01:53:26 +0000 2024</td>\n",
       "      <td>0</td>\n",
       "      <td>https://t.co/6AK5lTgWMG</td>\n",
       "      <td>1798896014994419875</td>\n",
       "      <td>https://pbs.twimg.com/media/GPb2ZHUaEAAdogy.jpg</td>\n",
       "      <td>NaN</td>\n",
       "      <td>zxx</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>https://x.com/anindyasinjai/status/17988960149...</td>\n",
       "      <td>2258214942</td>\n",
       "      <td>anindyasinjai</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5056</th>\n",
       "      <td>1798895997164351928</td>\n",
       "      <td>Fri Jun 07 01:53:22 +0000 2024</td>\n",
       "      <td>0</td>\n",
       "      <td>https://t.co/Bewddzz4Af</td>\n",
       "      <td>1798895997164351928</td>\n",
       "      <td>https://pbs.twimg.com/media/GPb2YCHaQAADJw1.jpg</td>\n",
       "      <td>NaN</td>\n",
       "      <td>zxx</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>https://x.com/anindyasinjai/status/17988959971...</td>\n",
       "      <td>2258214942</td>\n",
       "      <td>anindyasinjai</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5057</th>\n",
       "      <td>1798895948934082931</td>\n",
       "      <td>Fri Jun 07 01:53:10 +0000 2024</td>\n",
       "      <td>0</td>\n",
       "      <td>https://t.co/5WfO1dUzbw</td>\n",
       "      <td>1798895948934082931</td>\n",
       "      <td>https://pbs.twimg.com/media/GPb2VLvaoAAKs-X.jpg</td>\n",
       "      <td>NaN</td>\n",
       "      <td>zxx</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>https://x.com/anindyasinjai/status/17988959489...</td>\n",
       "      <td>2258214942</td>\n",
       "      <td>anindyasinjai</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5058</th>\n",
       "      <td>1798895926108692777</td>\n",
       "      <td>Fri Jun 07 01:53:05 +0000 2024</td>\n",
       "      <td>0</td>\n",
       "      <td>https://t.co/KexinQzY6b</td>\n",
       "      <td>1798895926108692777</td>\n",
       "      <td>https://pbs.twimg.com/media/GPb2T-5bQAATYL0.jpg</td>\n",
       "      <td>NaN</td>\n",
       "      <td>zxx</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>https://x.com/anindyasinjai/status/17988959261...</td>\n",
       "      <td>2258214942</td>\n",
       "      <td>anindyasinjai</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5059 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      conversation_id_str                      created_at  favorite_count  \\\n",
       "0     1801647536496857484  Fri Jun 14 21:55:26 +0000 2024              29   \n",
       "1     1801647536496857484  Fri Jun 14 21:52:02 +0000 2024               0   \n",
       "2     1801596984186302759  Fri Jun 14 21:50:38 +0000 2024               0   \n",
       "3     1801647536496857484  Fri Jun 14 21:48:47 +0000 2024               0   \n",
       "4     1801663965564489825  Fri Jun 14 21:43:06 +0000 2024               0   \n",
       "...                   ...                             ...             ...   \n",
       "5054  1798896044400595095  Fri Jun 07 01:53:33 +0000 2024               0   \n",
       "5055  1798896014994419875  Fri Jun 07 01:53:26 +0000 2024               0   \n",
       "5056  1798895997164351928  Fri Jun 07 01:53:22 +0000 2024               0   \n",
       "5057  1798895948934082931  Fri Jun 07 01:53:10 +0000 2024               0   \n",
       "5058  1798895926108692777  Fri Jun 07 01:53:05 +0000 2024               0   \n",
       "\n",
       "                                              full_text               id_str  \\\n",
       "0     @Indostransfer Asing Madura musim ini kemungki...  1801735224654762024   \n",
       "1     @Indostransfer Sebelumnya *cmiiw jika Persib l...  1801734368849002701   \n",
       "2            @FootyRankings Jeonbuk Port Lee man Persib  1801734016007344428   \n",
       "3     @Indostransfer @vikyfauzy Disini yg main bang ...  1801733549768511897   \n",
       "4     @swilkinsonbc Anjir naon warna biruuu persib m...  1801732118323605980   \n",
       "...                                                 ...                  ...   \n",
       "5054                            https://t.co/xBDsqdmcEI  1798896044400595095   \n",
       "5055                            https://t.co/6AK5lTgWMG  1798896014994419875   \n",
       "5056                            https://t.co/Bewddzz4Af  1798895997164351928   \n",
       "5057                            https://t.co/5WfO1dUzbw  1798895948934082931   \n",
       "5058                            https://t.co/KexinQzY6b  1798895926108692777   \n",
       "\n",
       "                                            image_url in_reply_to_screen_name  \\\n",
       "0                                                 NaN           Indostransfer   \n",
       "1                                                 NaN           Indostransfer   \n",
       "2                                                 NaN           FootyRankings   \n",
       "3                                                 NaN           Indostransfer   \n",
       "4                                                 NaN            swilkinsonbc   \n",
       "...                                               ...                     ...   \n",
       "5054  https://pbs.twimg.com/media/GPb2a2nbYAA3Bcs.jpg                     NaN   \n",
       "5055  https://pbs.twimg.com/media/GPb2ZHUaEAAdogy.jpg                     NaN   \n",
       "5056  https://pbs.twimg.com/media/GPb2YCHaQAADJw1.jpg                     NaN   \n",
       "5057  https://pbs.twimg.com/media/GPb2VLvaoAAKs-X.jpg                     NaN   \n",
       "5058  https://pbs.twimg.com/media/GPb2T-5bQAATYL0.jpg                     NaN   \n",
       "\n",
       "     lang location  quote_count  reply_count  retweet_count  \\\n",
       "0      in      NaN            0            5              0   \n",
       "1      in      NaN            0            0              0   \n",
       "2      in  Bandung            0            0              0   \n",
       "3      in      NaN            0            3              0   \n",
       "4      in      NaN            0            0              0   \n",
       "...   ...      ...          ...          ...            ...   \n",
       "5054  zxx      NaN            0            0              0   \n",
       "5055  zxx      NaN            0            0              0   \n",
       "5056  zxx      NaN            0            0              0   \n",
       "5057  zxx      NaN            0            0              0   \n",
       "5058  zxx      NaN            0            0              0   \n",
       "\n",
       "                                              tweet_url          user_id_str  \\\n",
       "0     https://x.com/wayuuu18/status/1801735224654762024  1387311427958886400   \n",
       "1     https://x.com/wayuuu18/status/1801734368849002701  1387311427958886400   \n",
       "2     https://x.com/vrijeman___/status/1801734016007...  1180727063264686080   \n",
       "3     https://x.com/hayesstyls/status/18017335497685...  1799981034425208832   \n",
       "4     https://x.com/Labeaufleur/status/1801732118323...  1544115184343531520   \n",
       "...                                                 ...                  ...   \n",
       "5054  https://x.com/anindyasinjai/status/17988960444...           2258214942   \n",
       "5055  https://x.com/anindyasinjai/status/17988960149...           2258214942   \n",
       "5056  https://x.com/anindyasinjai/status/17988959971...           2258214942   \n",
       "5057  https://x.com/anindyasinjai/status/17988959489...           2258214942   \n",
       "5058  https://x.com/anindyasinjai/status/17988959261...           2258214942   \n",
       "\n",
       "           username  \n",
       "0          wayuuu18  \n",
       "1          wayuuu18  \n",
       "2       vrijeman___  \n",
       "3        hayesstyls  \n",
       "4       Labeaufleur  \n",
       "...             ...  \n",
       "5054  anindyasinjai  \n",
       "5055  anindyasinjai  \n",
       "5056  anindyasinjai  \n",
       "5057  anindyasinjai  \n",
       "5058  anindyasinjai  \n",
       "\n",
       "[5059 rows x 15 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "file_path = '../08. Evaluasi Pola (Pattern Evaluation)/Dataset/persib.csv'\n",
    "\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\TEMP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import string\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from mpstemmer import MPStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import pickle\n",
    "\n",
    "# Download necessary NLTK data\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Initialize stemmer\n",
    "stemmer = MPStemmer()\n",
    "\n",
    "# Load the model\n",
    "model = load_model('../07. Pemodelan Data (Data Mining)/model/model-bilstm.h5')\n",
    "\n",
    "class Emotion:\n",
    "    @staticmethod\n",
    "    def classify_emotion(data):\n",
    "        def lower_case(text):\n",
    "            return text.lower()\n",
    "\n",
    "        def remove_tweet_special(text):\n",
    "            text = text.replace('\\\\t', \" \").replace('\\\\n', \" \").replace('\\\\u', \" \").replace('\\\\', \"\")\n",
    "            text = text.encode('ascii', 'replace').decode('ascii')\n",
    "            text = ' '.join(re.sub(r\"([@#][A-Za-z0-9]+)|(\\w+:\\/\\/\\S+)\", \" \", text).split())\n",
    "            return text.replace(\"http://\", \" \").replace(\"https://\", \" \")\n",
    "        \n",
    "        # fill na data kosong\n",
    "        def remove_empty_data(text):\n",
    "            if text == ' ' or text == '  ' or text == '   ':\n",
    "                return None\n",
    "            return text\n",
    "        \n",
    "\n",
    "        def remove_number(text):\n",
    "            return re.sub(r\"\\d+\", \"\", text)\n",
    "\n",
    "        def remove_punctuation(text):\n",
    "            return text.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "\n",
    "        def remove_whitespace_LT(text):\n",
    "            return text.strip()\n",
    "\n",
    "        def remove_whitespace_multiple(text):\n",
    "            return re.sub(r'\\s+', ' ', text)\n",
    "\n",
    "        def remove_singl_char(text):\n",
    "            return re.sub(r\"\\b[a-zA-Z]\\b\", \"\", text)\n",
    "\n",
    "        def remove_repeated_char(text):\n",
    "            return re.sub(r'(.)\\1+', r'\\1', text)\n",
    "\n",
    "        def word_tokenize_wrapper(text):\n",
    "            return word_tokenize(text)\n",
    "\n",
    "        normalizad_word = pd.read_csv(\"./utils/kamus-alay.csv\")\n",
    "        normalizad_word_dict = {row[0]: row[1] for index, row in normalizad_word.iterrows()}\n",
    "\n",
    "        def normalized_term(document):\n",
    "            return [normalizad_word_dict.get(term, term) for term in document]\n",
    "\n",
    "        def stem_wrapper(term):\n",
    "            return [stemmer.stem(word) for word in term]\n",
    "\n",
    "        stop_words = stopwords.words('indonesian')\n",
    "        stop_words = [word for word in stop_words if word not in ['tidak', 'baik', 'jelek', 'jangan', 'belum', 'bukan', \"enggak\", \"engga\", \"bener\", \"benar\"]]\n",
    "        stop_words.extend([\"yg\", \"dg\", \"rt\", \"dgn\", \"ny\", \"d\", 'klo', 'kalo', 'amp', 'biar', 'bikin', 'bilang', 'gak', 'ga', 'krn', 'nya', 'nih', 'sih', 'si', 'tau', 'tdk', 'tuh', 'utk', 'ya', 'jd', 'jgn', 'sdh', 'aja', 'n', 't', 'nyg', 'hehe', 'pen', 'u', 'nan', 'loh', 'rt', '&amp', 'yah'])\n",
    "\n",
    "        txt_stopword = pd.read_csv(\"utils/stopwords.txt\", names=[\"stopwords\"], header=None)\n",
    "        stop_words.extend(txt_stopword[\"stopwords\"][0].split(' '))\n",
    "        stop_words = set(stop_words)\n",
    "\n",
    "        def stopwords_removal(words):\n",
    "            return [word for word in words if word not in stop_words]\n",
    "\n",
    "        def replace_nan_with_none(data):\n",
    "            return data.applymap(lambda x: None if pd.isna(x) else x)\n",
    "\n",
    "        df = pd.DataFrame(data)\n",
    "\n",
    "        if 'predicted_label' not in df.columns:\n",
    "            df['predicted_label'] = np.nan\n",
    "            df['probability_emotion'] = np.nan\n",
    "\n",
    "        to_process_df = df[df['predicted_label'].isna()]\n",
    "\n",
    "        if not to_process_df.empty:\n",
    "            to_process_df['processed_text'] = to_process_df['full_text'].apply(lower_case)\n",
    "            to_process_df['processed_text'] = to_process_df['processed_text'].apply(remove_tweet_special)\n",
    "            to_process_df['processed_text'] = to_process_df['processed_text'].apply(remove_empty_data)\n",
    "            to_process_df['processed_text'] = to_process_df['processed_text'].apply(remove_number)\n",
    "            to_process_df['processed_text'] = to_process_df['processed_text'].apply(remove_punctuation)\n",
    "            to_process_df['processed_text'] = to_process_df['processed_text'].apply(remove_whitespace_LT)\n",
    "            to_process_df['processed_text'] = to_process_df['processed_text'].apply(remove_whitespace_multiple)\n",
    "            to_process_df['processed_text'] = to_process_df['processed_text'].apply(remove_singl_char)\n",
    "            to_process_df['processed_text'] = to_process_df['processed_text'].apply(remove_repeated_char)\n",
    "            to_process_df['processed_text'] = to_process_df['processed_text'].apply(word_tokenize_wrapper)\n",
    "            to_process_df['processed_text'] = to_process_df['processed_text'].apply(normalized_term)\n",
    "            to_process_df['processed_text'] = to_process_df['processed_text'].apply(stem_wrapper)\n",
    "            to_process_df['processed_text'] = to_process_df['processed_text'].apply(stopwords_removal)\n",
    "            to_process_df['processed_text'] = to_process_df['processed_text'].apply(' '.join)\n",
    "\n",
    "            print(\"Text preprocessing done!\")\n",
    "\n",
    "            with open('../07. Pemodelan Data (Data Mining)/tokenizer-emotion.pickle', 'rb') as handle:\n",
    "                tokenizer = pickle.load(handle)\n",
    "\n",
    "            sequences = tokenizer.texts_to_sequences(to_process_df['processed_text'])\n",
    "            padded_sequences = pad_sequences(sequences, maxlen=50, truncating='post', padding='post')\n",
    "\n",
    "            predictions = model.predict(padded_sequences)\n",
    "            \n",
    "            emotion_labels = ['Neutral', 'Anger', 'Joy', 'Love', 'Sad', 'Fear']\n",
    "            predicted_labels = []\n",
    "            predicted_probabilities = []\n",
    "\n",
    "            for pred in predictions:\n",
    "                max_idx = np.argmax(pred)\n",
    "                predicted_labels.append(emotion_labels[max_idx])\n",
    "                predicted_probabilities.append({emotion_labels[i]: pred[i] for i in range(len(emotion_labels))})\n",
    "\n",
    "            print(\"Prediction done!\")\n",
    "\n",
    "            to_process_df['predicted_label'] = predicted_labels\n",
    "            to_process_df['probability_emotion'] = [predicted_probabilities[i][predicted_labels[i]] for i in range(len(predicted_labels))]\n",
    "\n",
    "            df.update(to_process_df)\n",
    "\n",
    "        df = replace_nan_with_none(df)\n",
    "\n",
    "        return df\n",
    "    \n",
    "    @staticmethod\n",
    "    def calculate_emotion_percentages(data):\n",
    "        total = len(data)\n",
    "        emotion_counts = {'Neutral': 0, 'Anger': 1, 'Joy': 2, 'Love': 3, 'Sad': 4, 'Fear': 5}\n",
    "\n",
    "        for item in data:\n",
    "            emotion_counts[item['predicted_label']] += 1\n",
    "        \n",
    "        percentages = {emotion: (count / total) * 100 for emotion, count in emotion_counts.items()}\n",
    "        return percentages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\TEMP\\AppData\\Local\\Temp\\ipykernel_5668\\2180982358.py:64: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  normalizad_word_dict = {row[0]: row[1] for index, row in normalizad_word.iterrows()}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text preprocessing done!\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step\n",
      "Prediction done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\TEMP\\AppData\\Local\\Temp\\ipykernel_5668\\2180982358.py:134: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '['Anger' 'Neutral' 'Neutral' ... 'Love' 'Love' 'Love']' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  df.update(to_process_df)\n",
      "C:\\Users\\TEMP\\AppData\\Local\\Temp\\ipykernel_5668\\2180982358.py:84: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  return data.applymap(lambda x: None if pd.isna(x) else x)\n"
     ]
    }
   ],
   "source": [
    "# Convert float values to strings\n",
    "data['full_text'] = data['full_text'].astype(str)\n",
    "\n",
    "# Call the classify_emotion method\n",
    "hasil = Emotion.classify_emotion(data)\n",
    "\n",
    "# Menyimpan hasil hanya full_text, prediksi dan probabilitas \n",
    "hasil = hasil[['username','full_text','predicted_label', 'probability_emotion']]\n",
    "hasil.to_csv('hasil/hasil_prediksi-persib(bilstm).csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "skripsi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
