{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>conversation_id_str</th>\n",
       "      <th>created_at</th>\n",
       "      <th>favorite_count</th>\n",
       "      <th>full_text</th>\n",
       "      <th>id_str</th>\n",
       "      <th>lang</th>\n",
       "      <th>location</th>\n",
       "      <th>probability</th>\n",
       "      <th>quote_count</th>\n",
       "      <th>reply_count</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>topic</th>\n",
       "      <th>tweet_url</th>\n",
       "      <th>user_id_str</th>\n",
       "      <th>username</th>\n",
       "      <th>in_reply_to_screen_name</th>\n",
       "      <th>image_url</th>\n",
       "      <th>context</th>\n",
       "      <th>emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1800679330726715550</td>\n",
       "      <td>Tue Jun 11 23:59:42 +0000 2024</td>\n",
       "      <td>1</td>\n",
       "      <td>TIMNAS PRIA DAN WANITA MENANG + ERIK TEN HAG STAY</td>\n",
       "      <td>1800679330726715550</td>\n",
       "      <td>in</td>\n",
       "      <td>Indonesia</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>https://x.com/tigapuluhtigax/status/1800679330...</td>\n",
       "      <td>1188470356668895234</td>\n",
       "      <td>tigapuluhtigax</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Perjalanan timnas Indonesia di babak kualifika...</td>\n",
       "      <td>Joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1800679305925800015</td>\n",
       "      <td>Tue Jun 11 23:59:36 +0000 2024</td>\n",
       "      <td>1</td>\n",
       "      <td>ngerasa ga sih sekarang timnas kalo main pakek...</td>\n",
       "      <td>1800679305925800015</td>\n",
       "      <td>in</td>\n",
       "      <td>Jombang, Indonesia</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>https://x.com/sotongism/status/180067930592580...</td>\n",
       "      <td>961056397063409664</td>\n",
       "      <td>sotongism</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Anang sering disebut dalam konteks menyanyikan...</td>\n",
       "      <td>Fear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1800679288326476196</td>\n",
       "      <td>Tue Jun 11 23:59:31 +0000 2024</td>\n",
       "      <td>0</td>\n",
       "      <td>2 Sejarah Dicetak Timnas Indonesia Usai Ganyan...</td>\n",
       "      <td>1800679288326476196</td>\n",
       "      <td>in</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>https://x.com/peanutkacang123/status/180067928...</td>\n",
       "      <td>1464596330421100548</td>\n",
       "      <td>peanutkacang123</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Anang sering disebut dalam konteks menyanyikan...</td>\n",
       "      <td>Joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1800589420766355730</td>\n",
       "      <td>Tue Jun 11 23:59:28 +0000 2024</td>\n",
       "      <td>0</td>\n",
       "      <td>@womensfootie_id Tinggal tmbah pemain diaspora...</td>\n",
       "      <td>1800679273185047018</td>\n",
       "      <td>in</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>https://x.com/PKananku1927/status/180067927318...</td>\n",
       "      <td>1772880651248939008</td>\n",
       "      <td>PKananku1927</td>\n",
       "      <td>womensfootie_id</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Perjalanan timnas Indonesia di babak kualifika...</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1800679270857286043</td>\n",
       "      <td>Tue Jun 11 23:59:27 +0000 2024</td>\n",
       "      <td>0</td>\n",
       "      <td>Timnas Indonesia Vs Timnas Filipina 2-0 Indone...</td>\n",
       "      <td>1800679270857286043</td>\n",
       "      <td>in</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>https://x.com/kangsil2012/status/1800679270857...</td>\n",
       "      <td>546890263</td>\n",
       "      <td>kangsil2012</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Banyak pembicaraan tentang keberhasilan timnas...</td>\n",
       "      <td>Joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1875</th>\n",
       "      <td>1800537380392419660</td>\n",
       "      <td>Tue Jun 11 15:56:14 +0000 2024</td>\n",
       "      <td>0</td>\n",
       "      <td>@cingreborn aaak seneng banget guee #timnasday</td>\n",
       "      <td>1800557666047254585</td>\n",
       "      <td>in</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>https://x.com/kidleaderr/status/18005576660472...</td>\n",
       "      <td>1294225010924466177</td>\n",
       "      <td>kidleaderr</td>\n",
       "      <td>cingreborn</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Dukungan dari suporter untuk timnas Indonesia ...</td>\n",
       "      <td>Joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1876</th>\n",
       "      <td>1800557640936038793</td>\n",
       "      <td>Tue Jun 11 15:56:08 +0000 2024</td>\n",
       "      <td>0</td>\n",
       "      <td>PREDIKSI TOGEL OREGON 04 12 JUNI 2024 ANGKA MA...</td>\n",
       "      <td>1800557640936038793</td>\n",
       "      <td>in</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>https://x.com/yupitoto88/status/18005576409360...</td>\n",
       "      <td>1746107999226085376</td>\n",
       "      <td>yupitoto88</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://pbs.twimg.com/media/GPzdn55aMAA1CUe.png</td>\n",
       "      <td>Dukungan dari suporter untuk timnas Indonesia ...</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1877</th>\n",
       "      <td>1800557615824744466</td>\n",
       "      <td>Tue Jun 11 15:56:03 +0000 2024</td>\n",
       "      <td>0</td>\n",
       "      <td>Timnas menang tapi aku tetep kalah karo dee si...</td>\n",
       "      <td>1800557615824744466</td>\n",
       "      <td>in</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>https://x.com/Lalalan111111/status/18005576158...</td>\n",
       "      <td>1552276761194274816</td>\n",
       "      <td>Lalalan111111</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Dukungan dari suporter untuk timnas Indonesia ...</td>\n",
       "      <td>Sad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1878</th>\n",
       "      <td>1800557601232691595</td>\n",
       "      <td>Tue Jun 11 15:55:59 +0000 2024</td>\n",
       "      <td>0</td>\n",
       "      <td>buat mas mas belakang gue minta aidrop gol ked...</td>\n",
       "      <td>1800557601232691595</td>\n",
       "      <td>in</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>https://x.com/celestiaalsky/status/18005576012...</td>\n",
       "      <td>1761777254135394304</td>\n",
       "      <td>celestiaalsky</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Dukungan dari suporter untuk timnas Indonesia ...</td>\n",
       "      <td>Sad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1879</th>\n",
       "      <td>1800557593167020374</td>\n",
       "      <td>Tue Jun 11 15:55:57 +0000 2024</td>\n",
       "      <td>0</td>\n",
       "      <td>ada yg punya foto bareng pas awal main ga? #Ti...</td>\n",
       "      <td>1800557593167020374</td>\n",
       "      <td>in</td>\n",
       "      <td>bukan akun rp</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>https://x.com/mrshubnerr/status/18005575931670...</td>\n",
       "      <td>1641104358652903425</td>\n",
       "      <td>mrshubnerr</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Perjalanan timnas Indonesia di babak kualifika...</td>\n",
       "      <td>Joy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1880 rows × 19 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      conversation_id_str                      created_at  favorite_count  \\\n",
       "0     1800679330726715550  Tue Jun 11 23:59:42 +0000 2024               1   \n",
       "1     1800679305925800015  Tue Jun 11 23:59:36 +0000 2024               1   \n",
       "2     1800679288326476196  Tue Jun 11 23:59:31 +0000 2024               0   \n",
       "3     1800589420766355730  Tue Jun 11 23:59:28 +0000 2024               0   \n",
       "4     1800679270857286043  Tue Jun 11 23:59:27 +0000 2024               0   \n",
       "...                   ...                             ...             ...   \n",
       "1875  1800537380392419660  Tue Jun 11 15:56:14 +0000 2024               0   \n",
       "1876  1800557640936038793  Tue Jun 11 15:56:08 +0000 2024               0   \n",
       "1877  1800557615824744466  Tue Jun 11 15:56:03 +0000 2024               0   \n",
       "1878  1800557601232691595  Tue Jun 11 15:55:59 +0000 2024               0   \n",
       "1879  1800557593167020374  Tue Jun 11 15:55:57 +0000 2024               0   \n",
       "\n",
       "                                              full_text               id_str  \\\n",
       "0     TIMNAS PRIA DAN WANITA MENANG + ERIK TEN HAG STAY  1800679330726715550   \n",
       "1     ngerasa ga sih sekarang timnas kalo main pakek...  1800679305925800015   \n",
       "2     2 Sejarah Dicetak Timnas Indonesia Usai Ganyan...  1800679288326476196   \n",
       "3     @womensfootie_id Tinggal tmbah pemain diaspora...  1800679273185047018   \n",
       "4     Timnas Indonesia Vs Timnas Filipina 2-0 Indone...  1800679270857286043   \n",
       "...                                                 ...                  ...   \n",
       "1875     @cingreborn aaak seneng banget guee #timnasday  1800557666047254585   \n",
       "1876  PREDIKSI TOGEL OREGON 04 12 JUNI 2024 ANGKA MA...  1800557640936038793   \n",
       "1877  Timnas menang tapi aku tetep kalah karo dee si...  1800557615824744466   \n",
       "1878  buat mas mas belakang gue minta aidrop gol ked...  1800557601232691595   \n",
       "1879  ada yg punya foto bareng pas awal main ga? #Ti...  1800557593167020374   \n",
       "\n",
       "     lang            location  probability  quote_count  reply_count  \\\n",
       "0      in           Indonesia            0            0            0   \n",
       "1      in  Jombang, Indonesia            0            0            0   \n",
       "2      in                 NaN            0            0            0   \n",
       "3      in                 NaN            0            0            0   \n",
       "4      in                 NaN            0            0            0   \n",
       "...   ...                 ...          ...          ...          ...   \n",
       "1875   in                 NaN            0            0            0   \n",
       "1876   in                 NaN            0            0            0   \n",
       "1877   in                 NaN            0            0            0   \n",
       "1878   in                 NaN            0            0            1   \n",
       "1879   in       bukan akun rp            0            0            1   \n",
       "\n",
       "      retweet_count  topic                                          tweet_url  \\\n",
       "0                 0      1  https://x.com/tigapuluhtigax/status/1800679330...   \n",
       "1                 0      3  https://x.com/sotongism/status/180067930592580...   \n",
       "2                 0      3  https://x.com/peanutkacang123/status/180067928...   \n",
       "3                 0      1  https://x.com/PKananku1927/status/180067927318...   \n",
       "4                 0      2  https://x.com/kangsil2012/status/1800679270857...   \n",
       "...             ...    ...                                                ...   \n",
       "1875              0      0  https://x.com/kidleaderr/status/18005576660472...   \n",
       "1876              0      0  https://x.com/yupitoto88/status/18005576409360...   \n",
       "1877              0      0  https://x.com/Lalalan111111/status/18005576158...   \n",
       "1878              0      0  https://x.com/celestiaalsky/status/18005576012...   \n",
       "1879              0      1  https://x.com/mrshubnerr/status/18005575931670...   \n",
       "\n",
       "              user_id_str         username in_reply_to_screen_name  \\\n",
       "0     1188470356668895234   tigapuluhtigax                     NaN   \n",
       "1      961056397063409664        sotongism                     NaN   \n",
       "2     1464596330421100548  peanutkacang123                     NaN   \n",
       "3     1772880651248939008     PKananku1927         womensfootie_id   \n",
       "4               546890263      kangsil2012                     NaN   \n",
       "...                   ...              ...                     ...   \n",
       "1875  1294225010924466177       kidleaderr              cingreborn   \n",
       "1876  1746107999226085376       yupitoto88                     NaN   \n",
       "1877  1552276761194274816    Lalalan111111                     NaN   \n",
       "1878  1761777254135394304    celestiaalsky                     NaN   \n",
       "1879  1641104358652903425       mrshubnerr                     NaN   \n",
       "\n",
       "                                            image_url  \\\n",
       "0                                                 NaN   \n",
       "1                                                 NaN   \n",
       "2                                                 NaN   \n",
       "3                                                 NaN   \n",
       "4                                                 NaN   \n",
       "...                                               ...   \n",
       "1875                                              NaN   \n",
       "1876  https://pbs.twimg.com/media/GPzdn55aMAA1CUe.png   \n",
       "1877                                              NaN   \n",
       "1878                                              NaN   \n",
       "1879                                              NaN   \n",
       "\n",
       "                                                context  emotion  \n",
       "0     Perjalanan timnas Indonesia di babak kualifika...      Joy  \n",
       "1     Anang sering disebut dalam konteks menyanyikan...     Fear  \n",
       "2     Anang sering disebut dalam konteks menyanyikan...      Joy  \n",
       "3     Perjalanan timnas Indonesia di babak kualifika...  Neutral  \n",
       "4     Banyak pembicaraan tentang keberhasilan timnas...      Joy  \n",
       "...                                                 ...      ...  \n",
       "1875  Dukungan dari suporter untuk timnas Indonesia ...      Joy  \n",
       "1876  Dukungan dari suporter untuk timnas Indonesia ...  Neutral  \n",
       "1877  Dukungan dari suporter untuk timnas Indonesia ...      Sad  \n",
       "1878  Dukungan dari suporter untuk timnas Indonesia ...      Sad  \n",
       "1879  Perjalanan timnas Indonesia di babak kualifika...      Joy  \n",
       "\n",
       "[1880 rows x 19 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "file_path = '../09. Representasi Pengetahuan (Knowledge Representation)/sorted_topic_cluster_counts.csv'\n",
    "\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\TEMP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    }
   ],
   "source": [
    "# Import libraries yang diperlukan\n",
    "import string\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "import pickle\n",
    "from nltk.tokenize import word_tokenize\n",
    "from mpstemmer import MPStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Unduh stopwords dari NLTK\n",
    "nltk.download('stopwords')\n",
    "stemmer = MPStemmer()\n",
    "\n",
    "# Memuat model analisis emosi yang sudah dilatih\n",
    "model = load_model('../07. Pemodelan Data (Data Mining)/model/model-cnn.h5')\n",
    "\n",
    "class Emotion:\n",
    "    @staticmethod\n",
    "    def classify_emotion(data):\n",
    "\n",
    "        def lower_case(text):\n",
    "            return text.lower()\n",
    "\n",
    "        def remove_tweet_special(text):\n",
    "            text = text.replace('\\\\t', \" \").replace('\\\\n', \" \").replace('\\\\u', \" \").replace('\\\\', \"\")\n",
    "            text = text.encode('ascii', 'replace').decode('ascii')\n",
    "            text = ' '.join(re.sub(r\"([@#][A-Za-z0-9]+)|(\\w+:\\/\\/\\S+)\", \" \", text).split())\n",
    "            return text.replace(\"http://\", \" \").replace(\"https://\", \" \")\n",
    "\n",
    "        def remove_number(text):\n",
    "            return re.sub(r\"\\d+\", \"\", text)\n",
    "\n",
    "        def remove_punctuation(text):\n",
    "            return text.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "\n",
    "        def remove_whitespace_LT(text):\n",
    "            return text.strip()\n",
    "\n",
    "        def remove_whitespace_multiple(text):\n",
    "            return re.sub(r'\\s+', ' ', text)\n",
    "\n",
    "        def remove_singl_char(text):\n",
    "            return re.sub(r\"\\b[a-zA-Z]\\b\", \"\", text)\n",
    "\n",
    "        def remove_repeated_char(text):\n",
    "            return re.sub(r'(.)\\1+', r'\\1', text)\n",
    "\n",
    "        def word_tokenize_wrapper(text):\n",
    "            return word_tokenize(text)\n",
    "\n",
    "        normalizad_word = pd.read_csv(\"./utils/kamus-alay.csv\")\n",
    "        normalizad_word_dict = {}\n",
    "\n",
    "        for index, row in normalizad_word.iterrows():\n",
    "            if row[0] not in normalizad_word_dict:\n",
    "                normalizad_word_dict[row[0]] = row[1]\n",
    "\n",
    "        def normalized_term(document):\n",
    "            return [normalizad_word_dict[term] if term in normalizad_word_dict else term for term in document]\n",
    "\n",
    "        def stem_wrapper(term):\n",
    "            return [stemmer.stem(word) for word in term]\n",
    "\n",
    "        stop_words = stopwords.words('indonesian')\n",
    "        stop_words = [word for word in stop_words if word not in ['tidak', 'baik', 'jelek', 'jangan', 'belum', 'bukan', \"enggak\", \"engga\", \"bener\", \"benar\"]]\n",
    "        stop_words.extend([\"yg\", \"dg\", \"rt\", \"dgn\", \"ny\", \"d\", 'klo', 'kalo', 'amp', 'biar', 'bikin', 'bilang', \n",
    "                            'gak', 'ga', 'krn', 'nya', 'nih', 'sih', 'si', 'tau', 'tdk', 'tuh', 'utk', 'ya', \n",
    "                            'jd', 'jgn', 'sdh', 'aja', 'n', 't', 'nyg', 'hehe', 'pen', 'u', 'nan', 'loh', 'rt',\n",
    "                            '&amp', 'yah'])\n",
    "\n",
    "        txt_stopword = pd.read_csv(\"utils/stopwords.txt\", names=[\"stopwords\"], header=None)\n",
    "        stop_words.extend(txt_stopword[\"stopwords\"][0].split(' '))\n",
    "        stop_words = set(stop_words)\n",
    "\n",
    "        def stopwords_removal(words):\n",
    "            return [word for word in words if word not in stop_words]\n",
    "        \n",
    "        def replace_nan_with_none(data):\n",
    "            return data.applymap(lambda x: None if pd.isna(x) else x)\n",
    "\n",
    "        df = pd.DataFrame(data)\n",
    "\n",
    "        if 'predicted_label' not in df.columns:\n",
    "            df['predicted_label'] = np.nan\n",
    "            df['probability_emotion'] = np.nan\n",
    "\n",
    "        to_process_df = df[df['predicted_label'].isna()]\n",
    "\n",
    "        if not to_process_df.empty:\n",
    "            to_process_df['processed_text'] = to_process_df['full_text'].apply(lower_case)\n",
    "            to_process_df['processed_text'] = to_process_df['processed_text'].apply(remove_tweet_special)\n",
    "            to_process_df['processed_text'] = to_process_df['processed_text'].apply(remove_number)\n",
    "            to_process_df['processed_text'] = to_process_df['processed_text'].apply(remove_punctuation)\n",
    "            to_process_df['processed_text'] = to_process_df['processed_text'].apply(remove_whitespace_LT)\n",
    "            to_process_df['processed_text'] = to_process_df['processed_text'].apply(remove_whitespace_multiple)\n",
    "            to_process_df['processed_text'] = to_process_df['processed_text'].apply(remove_singl_char)\n",
    "            to_process_df['processed_text'] = to_process_df['processed_text'].apply(remove_repeated_char)\n",
    "            to_process_df['processed_text'] = to_process_df['processed_text'].apply(word_tokenize_wrapper)\n",
    "            to_process_df['processed_text'] = to_process_df['processed_text'].apply(normalized_term)\n",
    "            to_process_df['processed_text'] = to_process_df['processed_text'].apply(stem_wrapper)\n",
    "            to_process_df['processed_text'] = to_process_df['processed_text'].apply(stopwords_removal)\n",
    "            to_process_df['processed_text'] = to_process_df['processed_text'].apply(' '.join)\n",
    "\n",
    "            print(\"Text preprocessing done!\")\n",
    "\n",
    "            with open('../07. Pemodelan Data (Data Mining)/tokenizer-emotion(cnn).pickle', 'rb') as handle:\n",
    "                tokenizer = pickle.load(handle)\n",
    "\n",
    "            sequences = tokenizer.texts_to_sequences(to_process_df['processed_text'])\n",
    "            padded_sequences = pad_sequences(sequences, maxlen=64, truncating='post', padding='post')\n",
    "\n",
    "            predictions = model.predict(padded_sequences)\n",
    "            \n",
    "            emotion_labels = ['Neutral', 'Anger', 'Joy', 'Love', 'Sad', 'Fear']\n",
    "            predicted_labels = []\n",
    "            predicted_probabilities = []\n",
    "\n",
    "            for pred in predictions:\n",
    "                max_idx = np.argmax(pred)\n",
    "                predicted_labels.append(emotion_labels[max_idx])\n",
    "                predicted_probabilities.append({emotion_labels[i]: pred[i] for i in range(len(emotion_labels))})\n",
    "\n",
    "            print(\"Prediction done!\")\n",
    "\n",
    "            to_process_df['predicted_label'] = predicted_labels\n",
    "            to_process_df['probability_emotion'] = [predicted_probabilities[i][predicted_labels[i]] for i in range(len(predicted_labels))]\n",
    "\n",
    "            df.update(to_process_df)\n",
    "\n",
    "        df = replace_nan_with_none(df)\n",
    "\n",
    "        return df\n",
    "    \n",
    "    @staticmethod\n",
    "    def calculate_emotion_percentages(data):\n",
    "        total = len(data)\n",
    "        emotion_counts = {'Neutral': 0, 'Anger': 1, 'Joy': 2, 'Love': 3, 'Sad': 4, 'Fear': 5}\n",
    "\n",
    "        for item in data:\n",
    "            emotion_counts[item['predicted_label']] += 1\n",
    "        \n",
    "        percentages = {emotion: (count / total) * 100 for emotion, count in emotion_counts.items()}\n",
    "        return percentages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\TEMP\\AppData\\Local\\Temp\\ipykernel_15424\\3301233485.py:59: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  if row[0] not in normalizad_word_dict:\n",
      "C:\\Users\\TEMP\\AppData\\Local\\Temp\\ipykernel_15424\\3301233485.py:60: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  normalizad_word_dict[row[0]] = row[1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text preprocessing done!\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "Prediction done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\TEMP\\AppData\\Local\\Temp\\ipykernel_15424\\3301233485.py:132: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '['Love' 'Anger' 'Joy' ... 'Anger' 'Love' 'Neutral']' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  df.update(to_process_df)\n",
      "C:\\Users\\TEMP\\AppData\\Local\\Temp\\ipykernel_15424\\3301233485.py:83: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  return data.applymap(lambda x: None if pd.isna(x) else x)\n"
     ]
    }
   ],
   "source": [
    "# Mengklasifikasikan emosi\n",
    "hasil = Emotion.classify_emotion(data)\n",
    "\n",
    "# Menyimpan hasil hanya full_text, prediksi dan probabilitas \n",
    "hasil = hasil[['username','full_text','topic', 'predicted_label', 'probability_emotion']]\n",
    "hasil.to_csv('hasil_prediksi(cnn).csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "skripsi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
