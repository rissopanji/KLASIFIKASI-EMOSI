{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>conversation_id_str</th>\n",
       "      <th>created_at</th>\n",
       "      <th>favorite_count</th>\n",
       "      <th>full_text</th>\n",
       "      <th>id_str</th>\n",
       "      <th>image_url</th>\n",
       "      <th>in_reply_to_screen_name</th>\n",
       "      <th>lang</th>\n",
       "      <th>location</th>\n",
       "      <th>quote_count</th>\n",
       "      <th>reply_count</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>tweet_url</th>\n",
       "      <th>user_id_str</th>\n",
       "      <th>username</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1807201203832644018</td>\n",
       "      <td>Sat Jun 29 23:55:17 +0000 2024</td>\n",
       "      <td>0</td>\n",
       "      <td>@infomalang selamat pagi min minta tolong info...</td>\n",
       "      <td>1807201203832644018</td>\n",
       "      <td>NaN</td>\n",
       "      <td>infomalang</td>\n",
       "      <td>in</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>https://x.com/AzahraDean19363/status/180720120...</td>\n",
       "      <td>1710774704037089280</td>\n",
       "      <td>AzahraDean19363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1807040145256034783</td>\n",
       "      <td>Sat Jun 29 23:37:54 +0000 2024</td>\n",
       "      <td>0</td>\n",
       "      <td>@danu_kata @kaveits First impression utk yg ba...</td>\n",
       "      <td>1807196829161042011</td>\n",
       "      <td>NaN</td>\n",
       "      <td>danu_kata</td>\n",
       "      <td>in</td>\n",
       "      <td>Timbuktu</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>https://x.com/apiekluvdan/status/1807196829161...</td>\n",
       "      <td>1473134795341266950</td>\n",
       "      <td>apiekluvdan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1807192797939953929</td>\n",
       "      <td>Sat Jun 29 23:21:53 +0000 2024</td>\n",
       "      <td>1</td>\n",
       "      <td>Kursus Pendidikan Agama https://t.co/0YTwfKFUf0</td>\n",
       "      <td>1807192797939953929</td>\n",
       "      <td>https://pbs.twimg.com/media/GRRwRbIaEAEZiVm.jpg</td>\n",
       "      <td>NaN</td>\n",
       "      <td>in</td>\n",
       "      <td>Kuningan, Indonesia</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>https://x.com/ataulalaa/status/180719279793995...</td>\n",
       "      <td>742419990</td>\n",
       "      <td>ataulalaa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1806952429512233041</td>\n",
       "      <td>Sat Jun 29 22:56:05 +0000 2024</td>\n",
       "      <td>1</td>\n",
       "      <td>@imminenotyours_ @innovacommunity Biasanya bel...</td>\n",
       "      <td>1807186304561758344</td>\n",
       "      <td>NaN</td>\n",
       "      <td>imminenotyours_</td>\n",
       "      <td>in</td>\n",
       "      <td>Colorado, USA</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>https://x.com/dabudabu6969/status/180718630456...</td>\n",
       "      <td>711377077916794880</td>\n",
       "      <td>dabudabu6969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1806952429512233041</td>\n",
       "      <td>Sat Jun 29 22:54:41 +0000 2024</td>\n",
       "      <td>0</td>\n",
       "      <td>@innovacommunity Paling bener ikut kursus dah</td>\n",
       "      <td>1807185953393721541</td>\n",
       "      <td>NaN</td>\n",
       "      <td>innovacommunity</td>\n",
       "      <td>in</td>\n",
       "      <td>Colorado, USA</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>https://x.com/dabudabu6969/status/180718595339...</td>\n",
       "      <td>711377077916794880</td>\n",
       "      <td>dabudabu6969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1235</th>\n",
       "      <td>1804304762684477570</td>\n",
       "      <td>Sat Jun 22 00:05:52 +0000 2024</td>\n",
       "      <td>0</td>\n",
       "      <td>Assalamualaikum &amp;amp; Good morning Have a bles...</td>\n",
       "      <td>1804304762684477570</td>\n",
       "      <td>https://pbs.twimg.com/media/GQotnpLakAUHJm_.jpg</td>\n",
       "      <td>NaN</td>\n",
       "      <td>in</td>\n",
       "      <td>Muar, Johor</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>https://x.com/APemakanan/status/18043047626844...</td>\n",
       "      <td>1278486349599748097</td>\n",
       "      <td>APemakanan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1236</th>\n",
       "      <td>1804299666043080977</td>\n",
       "      <td>Sat Jun 22 00:01:15 +0000 2024</td>\n",
       "      <td>0</td>\n",
       "      <td>@Piyu_chann tempat kursus mah gak kenal tutup ...</td>\n",
       "      <td>1804303602141532583</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Piyu_chann</td>\n",
       "      <td>in</td>\n",
       "      <td>Jakarta Capital Region</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>https://x.com/kesurupaaan/status/1804303602141...</td>\n",
       "      <td>1577180584031371265</td>\n",
       "      <td>kesurupaaan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1237</th>\n",
       "      <td>1804116632228499520</td>\n",
       "      <td>Fri Jun 21 23:55:55 +0000 2024</td>\n",
       "      <td>0</td>\n",
       "      <td>@bturn2c aku pernah tapi kurang ngerti paket c...</td>\n",
       "      <td>1804302258194256144</td>\n",
       "      <td>NaN</td>\n",
       "      <td>bibequwu</td>\n",
       "      <td>in</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>https://x.com/ggelatou/status/1804302258194256144</td>\n",
       "      <td>1513078233574834176</td>\n",
       "      <td>ggelatou</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1238</th>\n",
       "      <td>1804148890708681061</td>\n",
       "      <td>Fri Jun 21 23:50:22 +0000 2024</td>\n",
       "      <td>1</td>\n",
       "      <td>@tanyarlfes Daripada joki mending buka kursus ...</td>\n",
       "      <td>1804300861398749310</td>\n",
       "      <td>NaN</td>\n",
       "      <td>tanyarlfes</td>\n",
       "      <td>in</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>https://x.com/kecoabunting/status/180430086139...</td>\n",
       "      <td>611902187</td>\n",
       "      <td>kecoabunting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1239</th>\n",
       "      <td>1804294931500105759</td>\n",
       "      <td>Fri Jun 21 23:26:48 +0000 2024</td>\n",
       "      <td>0</td>\n",
       "      <td>Tu lah dia kursus kahwin lebai low class duk c...</td>\n",
       "      <td>1804294931500105759</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>in</td>\n",
       "      <td>Selangor, Malaysia</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>https://x.com/zaphree/status/1804294931500105759</td>\n",
       "      <td>233585762</td>\n",
       "      <td>zaphree</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1240 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      conversation_id_str                      created_at  favorite_count  \\\n",
       "0     1807201203832644018  Sat Jun 29 23:55:17 +0000 2024               0   \n",
       "1     1807040145256034783  Sat Jun 29 23:37:54 +0000 2024               0   \n",
       "2     1807192797939953929  Sat Jun 29 23:21:53 +0000 2024               1   \n",
       "3     1806952429512233041  Sat Jun 29 22:56:05 +0000 2024               1   \n",
       "4     1806952429512233041  Sat Jun 29 22:54:41 +0000 2024               0   \n",
       "...                   ...                             ...             ...   \n",
       "1235  1804304762684477570  Sat Jun 22 00:05:52 +0000 2024               0   \n",
       "1236  1804299666043080977  Sat Jun 22 00:01:15 +0000 2024               0   \n",
       "1237  1804116632228499520  Fri Jun 21 23:55:55 +0000 2024               0   \n",
       "1238  1804148890708681061  Fri Jun 21 23:50:22 +0000 2024               1   \n",
       "1239  1804294931500105759  Fri Jun 21 23:26:48 +0000 2024               0   \n",
       "\n",
       "                                              full_text               id_str  \\\n",
       "0     @infomalang selamat pagi min minta tolong info...  1807201203832644018   \n",
       "1     @danu_kata @kaveits First impression utk yg ba...  1807196829161042011   \n",
       "2       Kursus Pendidikan Agama https://t.co/0YTwfKFUf0  1807192797939953929   \n",
       "3     @imminenotyours_ @innovacommunity Biasanya bel...  1807186304561758344   \n",
       "4         @innovacommunity Paling bener ikut kursus dah  1807185953393721541   \n",
       "...                                                 ...                  ...   \n",
       "1235  Assalamualaikum &amp; Good morning Have a bles...  1804304762684477570   \n",
       "1236  @Piyu_chann tempat kursus mah gak kenal tutup ...  1804303602141532583   \n",
       "1237  @bturn2c aku pernah tapi kurang ngerti paket c...  1804302258194256144   \n",
       "1238  @tanyarlfes Daripada joki mending buka kursus ...  1804300861398749310   \n",
       "1239  Tu lah dia kursus kahwin lebai low class duk c...  1804294931500105759   \n",
       "\n",
       "                                            image_url in_reply_to_screen_name  \\\n",
       "0                                                 NaN              infomalang   \n",
       "1                                                 NaN               danu_kata   \n",
       "2     https://pbs.twimg.com/media/GRRwRbIaEAEZiVm.jpg                     NaN   \n",
       "3                                                 NaN         imminenotyours_   \n",
       "4                                                 NaN         innovacommunity   \n",
       "...                                               ...                     ...   \n",
       "1235  https://pbs.twimg.com/media/GQotnpLakAUHJm_.jpg                     NaN   \n",
       "1236                                              NaN              Piyu_chann   \n",
       "1237                                              NaN                bibequwu   \n",
       "1238                                              NaN              tanyarlfes   \n",
       "1239                                              NaN                     NaN   \n",
       "\n",
       "     lang                location  quote_count  reply_count  retweet_count  \\\n",
       "0      in                     NaN            0            2              0   \n",
       "1      in                Timbuktu            0            1              0   \n",
       "2      in     Kuningan, Indonesia            0            0              0   \n",
       "3      in           Colorado, USA            0            0              0   \n",
       "4      in           Colorado, USA            0            0              0   \n",
       "...   ...                     ...          ...          ...            ...   \n",
       "1235   in             Muar, Johor            0            0              0   \n",
       "1236   in  Jakarta Capital Region            0            1              0   \n",
       "1237   in                     NaN            0            0              0   \n",
       "1238   in                     NaN            0            0              0   \n",
       "1239   in      Selangor, Malaysia            0            1              0   \n",
       "\n",
       "                                              tweet_url          user_id_str  \\\n",
       "0     https://x.com/AzahraDean19363/status/180720120...  1710774704037089280   \n",
       "1     https://x.com/apiekluvdan/status/1807196829161...  1473134795341266950   \n",
       "2     https://x.com/ataulalaa/status/180719279793995...            742419990   \n",
       "3     https://x.com/dabudabu6969/status/180718630456...   711377077916794880   \n",
       "4     https://x.com/dabudabu6969/status/180718595339...   711377077916794880   \n",
       "...                                                 ...                  ...   \n",
       "1235  https://x.com/APemakanan/status/18043047626844...  1278486349599748097   \n",
       "1236  https://x.com/kesurupaaan/status/1804303602141...  1577180584031371265   \n",
       "1237  https://x.com/ggelatou/status/1804302258194256144  1513078233574834176   \n",
       "1238  https://x.com/kecoabunting/status/180430086139...            611902187   \n",
       "1239   https://x.com/zaphree/status/1804294931500105759            233585762   \n",
       "\n",
       "             username  \n",
       "0     AzahraDean19363  \n",
       "1         apiekluvdan  \n",
       "2           ataulalaa  \n",
       "3        dabudabu6969  \n",
       "4        dabudabu6969  \n",
       "...               ...  \n",
       "1235       APemakanan  \n",
       "1236      kesurupaaan  \n",
       "1237         ggelatou  \n",
       "1238     kecoabunting  \n",
       "1239          zaphree  \n",
       "\n",
       "[1240 rows x 15 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "file_path = '../08. Evaluasi Pola (Pattern Evaluation)/Dataset/kursus.csv'\n",
    "\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "partially initialized module 'nltk' has no attribute 'data' (most likely due to a circular import)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mre\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpickle\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtokenize\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m word_tokenize\n",
      "File \u001b[1;32mc:\\Users\\TEMP\\miniconda3\\envs\\skripsi\\Lib\\site-packages\\nltk\\__init__.py:153\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtag\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m    152\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtokenize\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m--> 153\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtranslate\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m    154\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtree\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m    155\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msem\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\TEMP\\miniconda3\\envs\\skripsi\\Lib\\site-packages\\nltk\\translate\\__init__.py:24\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtranslate\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbleu_score\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m sentence_bleu \u001b[38;5;28;01mas\u001b[39;00m bleu\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtranslate\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mribes_score\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m sentence_ribes \u001b[38;5;28;01mas\u001b[39;00m ribes\n\u001b[1;32m---> 24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtranslate\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmeteor_score\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m meteor_score \u001b[38;5;28;01mas\u001b[39;00m meteor\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtranslate\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m alignment_error_rate\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtranslate\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstack_decoder\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m StackDecoder\n",
      "File \u001b[1;32mc:\\Users\\TEMP\\miniconda3\\envs\\skripsi\\Lib\\site-packages\\nltk\\translate\\meteor_score.py:13\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mitertools\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m chain, product\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Callable, Iterable, List, Tuple\n\u001b[1;32m---> 13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcorpus\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m WordNetCorpusReader, wordnet\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstem\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m StemmerI\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstem\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mporter\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PorterStemmer\n",
      "File \u001b[1;32mc:\\Users\\TEMP\\miniconda3\\envs\\skripsi\\Lib\\site-packages\\nltk\\corpus\\__init__.py:64\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;124;03mNLTK corpus readers.  The modules in this package provide functions\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;124;03mthat can be used to read corpus files in a variety of formats.  These\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     59\u001b[0m \n\u001b[0;32m     60\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mre\u001b[39;00m\n\u001b[1;32m---> 64\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcorpus\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mreader\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcorpus\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LazyCorpusLoader\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtokenize\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RegexpTokenizer\n",
      "File \u001b[1;32mc:\\Users\\TEMP\\miniconda3\\envs\\skripsi\\Lib\\site-packages\\nltk\\corpus\\reader\\__init__.py:57\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Natural Language Toolkit: Corpus Readers\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Copyright (C) 2001-2023 NLTK Project\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# URL: <https://www.nltk.org/>\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# For license information, see LICENSE.TXT\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;124;03mNLTK corpus readers.  The modules in this package provide functions\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;124;03mthat can be used to read corpus fileids in a variety of formats.  These\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;124;03misort:skip_file\u001b[39;00m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m---> 57\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcorpus\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mreader\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mplaintext\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcorpus\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mreader\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcorpus\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mreader\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\TEMP\\miniconda3\\envs\\skripsi\\Lib\\site-packages\\nltk\\corpus\\reader\\plaintext.py:20\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcorpus\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mreader\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtokenize\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m---> 20\u001b[0m \u001b[38;5;28;43;01mclass\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;21;43;01mPlaintextCorpusReader\u001b[39;49;00m\u001b[43m(\u001b[49m\u001b[43mCorpusReader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;250;43m    \u001b[39;49m\u001b[38;5;124;43;03m\"\"\"\u001b[39;49;00m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;124;43;03m    Reader for corpora that consist of plaintext documents.  Paragraphs\u001b[39;49;00m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;124;43;03m    are assumed to be split using blank lines.  Sentences and words can\u001b[39;49;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;124;43;03m    overriding the ``CorpusView`` class variable.\u001b[39;49;00m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;124;43;03m    \"\"\"\u001b[39;49;00m\n\u001b[0;32m     32\u001b[0m \u001b[43m    \u001b[49m\u001b[43mCorpusView\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mStreamBackedCorpusView\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\TEMP\\miniconda3\\envs\\skripsi\\Lib\\site-packages\\nltk\\corpus\\reader\\plaintext.py:42\u001b[0m, in \u001b[0;36mPlaintextCorpusReader\u001b[1;34m()\u001b[0m\n\u001b[0;32m     32\u001b[0m CorpusView \u001b[38;5;241m=\u001b[39m StreamBackedCorpusView\n\u001b[0;32m     33\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"The corpus view class used by this reader.  Subclasses of\u001b[39;00m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;124;03m   ``PlaintextCorpusReader`` may specify alternative corpus view\u001b[39;00m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;124;03m   classes (e.g., to skip the preface sections of documents.)\"\"\"\u001b[39;00m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[0;32m     38\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m     39\u001b[0m     root,\n\u001b[0;32m     40\u001b[0m     fileids,\n\u001b[0;32m     41\u001b[0m     word_tokenizer\u001b[38;5;241m=\u001b[39mWordPunctTokenizer(),\n\u001b[1;32m---> 42\u001b[0m     sent_tokenizer\u001b[38;5;241m=\u001b[39m\u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241m.\u001b[39mLazyLoader(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtokenizers/punkt/english.pickle\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m     43\u001b[0m     para_block_reader\u001b[38;5;241m=\u001b[39mread_blankline_block,\n\u001b[0;32m     44\u001b[0m     encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf8\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     45\u001b[0m ):\n\u001b[0;32m     46\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;124;03m    Construct a new plaintext corpus reader for a set of documents\u001b[39;00m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;124;03m    located at the given root directory.  Example usage:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;124;03m        corpus into paragraph blocks.\u001b[39;00m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m     62\u001b[0m     CorpusReader\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, root, fileids, encoding)\n",
      "\u001b[1;31mAttributeError\u001b[0m: partially initialized module 'nltk' has no attribute 'data' (most likely due to a circular import)"
     ]
    }
   ],
   "source": [
    "# Import libraries yang diperlukan\n",
    "import string\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "import pickle\n",
    "from nltk.tokenize import word_tokenize\n",
    "from mpstemmer import MPStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Unduh stopwords dari NLTK\n",
    "nltk.download('stopwords')\n",
    "stemmer = MPStemmer()\n",
    "\n",
    "# Memuat model analisis emosi yang sudah dilatih\n",
    "model = load_model('../07. Pemodelan Data (Data Mining)/model/model-cnn.h5')\n",
    "\n",
    "class Emotion:\n",
    "    @staticmethod\n",
    "    def classify_emotion(data):\n",
    "\n",
    "        def lower_case(text):\n",
    "            return text.lower()\n",
    "\n",
    "        def remove_tweet_special(text):\n",
    "            text = text.replace('\\\\t', \" \").replace('\\\\n', \" \").replace('\\\\u', \" \").replace('\\\\', \"\")\n",
    "            text = text.encode('ascii', 'replace').decode('ascii')\n",
    "            text = ' '.join(re.sub(r\"([@#][A-Za-z0-9]+)|(\\w+:\\/\\/\\S+)\", \" \", text).split())\n",
    "            return text.replace(\"http://\", \" \").replace(\"https://\", \" \")\n",
    "\n",
    "        def remove_number(text):\n",
    "            return re.sub(r\"\\d+\", \"\", text)\n",
    "\n",
    "        def remove_punctuation(text):\n",
    "            return text.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "\n",
    "        def remove_whitespace_LT(text):\n",
    "            return text.strip()\n",
    "\n",
    "        def remove_whitespace_multiple(text):\n",
    "            return re.sub(r'\\s+', ' ', text)\n",
    "\n",
    "        def remove_singl_char(text):\n",
    "            return re.sub(r\"\\b[a-zA-Z]\\b\", \"\", text)\n",
    "\n",
    "        def remove_repeated_char(text):\n",
    "            return re.sub(r'(.)\\1+', r'\\1', text)\n",
    "\n",
    "        def word_tokenize_wrapper(text):\n",
    "            return word_tokenize(text)\n",
    "\n",
    "        normalizad_word = pd.read_csv(\"./utils/kamus-alay.csv\")\n",
    "        normalizad_word_dict = {}\n",
    "\n",
    "        for index, row in normalizad_word.iterrows():\n",
    "            if row[0] not in normalizad_word_dict:\n",
    "                normalizad_word_dict[row[0]] = row[1]\n",
    "\n",
    "        def normalized_term(document):\n",
    "            return [normalizad_word_dict[term] if term in normalizad_word_dict else term for term in document]\n",
    "\n",
    "        def stem_wrapper(term):\n",
    "            return [stemmer.stem(word) for word in term]\n",
    "\n",
    "        stop_words = stopwords.words('indonesian')\n",
    "        stop_words = [word for word in stop_words if word not in ['tidak', 'baik', 'jelek', 'jangan', 'belum', 'bukan', \"enggak\", \"engga\", \"bener\", \"benar\"]]\n",
    "        stop_words.extend([\"yg\", \"dg\", \"rt\", \"dgn\", \"ny\", \"d\", 'klo', 'kalo', 'amp', 'biar', 'bikin', 'bilang', \n",
    "                            'gak', 'ga', 'krn', 'nya', 'nih', 'sih', 'si', 'tau', 'tdk', 'tuh', 'utk', 'ya', \n",
    "                            'jd', 'jgn', 'sdh', 'aja', 'n', 't', 'nyg', 'hehe', 'pen', 'u', 'nan', 'loh', 'rt',\n",
    "                            '&amp', 'yah'])\n",
    "\n",
    "        txt_stopword = pd.read_csv(\"utils/stopwords.txt\", names=[\"stopwords\"], header=None)\n",
    "        stop_words.extend(txt_stopword[\"stopwords\"][0].split(' '))\n",
    "        stop_words = set(stop_words)\n",
    "\n",
    "        def stopwords_removal(words):\n",
    "            return [word for word in words if word not in stop_words]\n",
    "        \n",
    "        def replace_nan_with_none(data):\n",
    "            return data.applymap(lambda x: None if pd.isna(x) else x)\n",
    "\n",
    "        df = pd.DataFrame(data)\n",
    "\n",
    "        if 'predicted_label' not in df.columns:\n",
    "            df['predicted_label'] = np.nan\n",
    "            df['probability_emotion'] = np.nan\n",
    "\n",
    "        to_process_df = df[df['predicted_label'].isna()]\n",
    "\n",
    "        if not to_process_df.empty:\n",
    "            to_process_df['processed_text'] = to_process_df['full_text'].apply(lower_case)\n",
    "            to_process_df['processed_text'] = to_process_df['processed_text'].apply(remove_tweet_special)\n",
    "            to_process_df['processed_text'] = to_process_df['processed_text'].apply(remove_number)\n",
    "            to_process_df['processed_text'] = to_process_df['processed_text'].apply(remove_punctuation)\n",
    "            to_process_df['processed_text'] = to_process_df['processed_text'].apply(remove_whitespace_LT)\n",
    "            to_process_df['processed_text'] = to_process_df['processed_text'].apply(remove_whitespace_multiple)\n",
    "            to_process_df['processed_text'] = to_process_df['processed_text'].apply(remove_singl_char)\n",
    "            to_process_df['processed_text'] = to_process_df['processed_text'].apply(remove_repeated_char)\n",
    "            to_process_df['processed_text'] = to_process_df['processed_text'].apply(word_tokenize_wrapper)\n",
    "            to_process_df['processed_text'] = to_process_df['processed_text'].apply(normalized_term)\n",
    "            to_process_df['processed_text'] = to_process_df['processed_text'].apply(stem_wrapper)\n",
    "            to_process_df['processed_text'] = to_process_df['processed_text'].apply(stopwords_removal)\n",
    "            to_process_df['processed_text'] = to_process_df['processed_text'].apply(' '.join)\n",
    "\n",
    "            print(\"Text preprocessing done!\")\n",
    "\n",
    "            with open('../07. Pemodelan Data (Data Mining)/tokenizer-emotion.pickle', 'rb') as handle:\n",
    "                tokenizer = pickle.load(handle)\n",
    "\n",
    "            sequences = tokenizer.texts_to_sequences(to_process_df['processed_text'])\n",
    "            padded_sequences = pad_sequences(sequences, maxlen=64, truncating='post', padding='post')\n",
    "\n",
    "            predictions = model.predict(padded_sequences)\n",
    "            \n",
    "            emotion_labels = ['Neutral', 'Anger', 'Joy', 'Love', 'Sad', 'Fear']\n",
    "            predicted_labels = []\n",
    "            predicted_probabilities = []\n",
    "\n",
    "            for pred in predictions:\n",
    "                max_idx = np.argmax(pred)\n",
    "                predicted_labels.append(emotion_labels[max_idx])\n",
    "                predicted_probabilities.append({emotion_labels[i]: pred[i] for i in range(len(emotion_labels))})\n",
    "\n",
    "            print(\"Prediction done!\")\n",
    "\n",
    "            to_process_df['predicted_label'] = predicted_labels\n",
    "            to_process_df['probability_emotion'] = [predicted_probabilities[i][predicted_labels[i]] for i in range(len(predicted_labels))]\n",
    "\n",
    "            df.update(to_process_df)\n",
    "\n",
    "        df = replace_nan_with_none(df)\n",
    "\n",
    "        return df\n",
    "    \n",
    "    @staticmethod\n",
    "    def calculate_emotion_percentages(data):\n",
    "        total = len(data)\n",
    "        emotion_counts = {'Neutral': 0, 'Anger': 1, 'Joy': 2, 'Love': 3, 'Sad': 4, 'Fear': 5}\n",
    "\n",
    "        for item in data:\n",
    "            emotion_counts[item['predicted_label']] += 1\n",
    "        \n",
    "        percentages = {emotion: (count / total) * 100 for emotion, count in emotion_counts.items()}\n",
    "        return percentages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\TEMP\\AppData\\Local\\Temp\\ipykernel_120\\498368751.py:59: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  if row[0] not in normalizad_word_dict:\n",
      "C:\\Users\\TEMP\\AppData\\Local\\Temp\\ipykernel_120\\498368751.py:60: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  normalizad_word_dict[row[0]] = row[1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text preprocessing done!\n",
      "\u001b[1m194/194\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
      "Prediction done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\TEMP\\AppData\\Local\\Temp\\ipykernel_120\\498368751.py:132: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '['Anger' 'Anger' 'Joy' ... 'Joy' 'Joy' 'Joy']' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  df.update(to_process_df)\n",
      "C:\\Users\\TEMP\\AppData\\Local\\Temp\\ipykernel_120\\498368751.py:83: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  return data.applymap(lambda x: None if pd.isna(x) else x)\n"
     ]
    }
   ],
   "source": [
    "# Mengklasifikasikan emosi\n",
    "hasil = Emotion.classify_emotion(data)\n",
    "\n",
    "# Menyimpan hasil hanya full_text, prediksi dan probabilitas \n",
    "hasil = hasil[['username','full_text','topic','topic_probability','predicted_label', 'probability_emotion']]\n",
    "hasil.to_csv('hasil/hasil_prediksi-kursus(cnn).csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "skripsi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
