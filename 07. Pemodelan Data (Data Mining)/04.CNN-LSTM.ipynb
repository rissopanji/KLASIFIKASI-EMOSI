{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import ast\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Tensorflow version:\", tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_path ='../07. Pemodelan Data (Data Mining)/dataset/Dataset_clean(penelitian).csv'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(train_df_path, sep=',', header=None, usecols=[1,2], skiprows=1)\n",
    "\n",
    "train_df.columns = ['full_text', 'emotion']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_label_idx(label):\n",
    "    if label == 'Fear':\n",
    "        return 5\n",
    "    if label == 'Sad':\n",
    "        return 4\n",
    "    if label == 'Love':\n",
    "        return 3\n",
    "    if label == 'Joy':\n",
    "        return 2\n",
    "    if label == 'Anger':\n",
    "        return 1\n",
    "    if label == 'Neutral':\n",
    "        return 0\n",
    "train_df['emotion'] = train_df['emotion'].apply(get_label_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function to join the list of tokens as a single document string\n",
    "def join_text_list(texts):\n",
    "    try:\n",
    "        texts = ast.literal_eval(texts)\n",
    "        return ' '.join([text for text in texts])\n",
    "    except (ValueError, SyntaxError):\n",
    "        return None  # or return an empty string if you prefer\n",
    "\n",
    "# Apply the function to the 'full_text' column\n",
    "train_df[\"full_text\"] = train_df[\"full_text\"].apply(join_text_list)\n",
    "\n",
    "# Display the first few rows of the preprocessed column\n",
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jumlah_netral = len(train_df[train_df['emotion'] == 0])\n",
    "jumlah_marah = len(train_df[train_df['emotion'] == 1])\n",
    "jumlah_senang = len(train_df[train_df['emotion'] == 2])\n",
    "jumlah_cinta = len(train_df[train_df['emotion'] == 3])\n",
    "jumlah_sedih = len(train_df[train_df['emotion'] == 4])\n",
    "jumlah_takut = len(train_df[train_df['emotion'] == 5])\n",
    "\n",
    "print(\"Jumlah Netral: \", jumlah_netral)\n",
    "print(\"Jumlah Marah: \", jumlah_marah)\n",
    "print(\"Jumlah Senang: \", jumlah_senang)\n",
    "print(\"Jumlah Cinta: \", jumlah_cinta)\n",
    "print(\"Jumlah Sedih: \", jumlah_sedih)\n",
    "print(\"Jumlah Takut: \", jumlah_takut)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset menjadi fitur (X) dan label (y)\n",
    "X_train = train_df['full_text']\n",
    "y_train = train_df['emotion']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##split dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape, X_valid.shape, y_train.shape, y_valid.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Padded Sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "\n",
    "vocab_size = 60000 #ukuran kosakata yang akan digunakan\n",
    "max_length = 50 #panjang maksimal dari sebuah kalimat\n",
    "embedding_dim = 8 #dimensi vektor embedding yang akan digunakan\n",
    "trunc_type='post' #jika panjang kalimat melebihi max_length, potong bagian belakang kalimat\n",
    "oov_tok = \"<OOV>\" #OOV (Out Of Vocabulary) token, jika suatu kata tidak ditemukan dalam kosakata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Define the \"all_texts\" variable\n",
    "X_train = [str(text) for text in X_train]\n",
    "X_valid = [str(text) for text in X_valid]\n",
    "\n",
    "# Tokenisasi\n",
    "tokenizer = Tokenizer(num_words=vocab_size, oov_token=oov_tok)\n",
    "\n",
    "# Gabungkan semua teks untuk membuat kamus indeks kata\n",
    "all_texts = list(X_train) + list(X_valid)\n",
    "\n",
    "# Membuat kamus indeks kata\n",
    "tokenizer.fit_on_texts(all_texts)\n",
    "word_index = tokenizer.word_index\n",
    "word_index\n",
    "\n",
    "\n",
    "# Konversi kalimat menjadi urutan kata\n",
    "train_sequences = tokenizer.texts_to_sequences(X_train)\n",
    "train_padded = pad_sequences(train_sequences, maxlen=max_length, truncating=trunc_type, padding=\"post\")\n",
    "\n",
    "valid_sequences = tokenizer.texts_to_sequences(X_valid)\n",
    "valid_padded = pad_sequences(valid_sequences, maxlen=max_length, truncating=trunc_type, padding=\"post\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### kamus data untuk api Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "\n",
    "# Simpan tokenizer ke dalam file\n",
    "# with open('tokenizer-2classes-no-stemming.pickle', 'wb') as handle:\n",
    "#     pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_padded[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Gabungkan data pelatihan dan validasi\n",
    "sentences = [sentence.split() for sentence in X_train] + [sentence.split() for sentence in X_valid]\n",
    "\n",
    "# Latih model Word2Vec\n",
    "w2v_model = Word2Vec(sentences, vector_size=8, window=5, min_count=1, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a weight matrix for the embedding layer\n",
    "embedding_matrix = np.zeros((vocab_size, 8))\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    if word in w2v_model.wv:\n",
    "        embedding_matrix[i] = w2v_model.wv[word]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mencari kata berdasarkan vektor tertentu\n",
    "def find_word_by_vector(embedding_matrix, vector):\n",
    "    for word, index in word_index.items():\n",
    "        if np.array_equal(embedding_matrix[index], vector):\n",
    "            return word, embedding_matrix[index]\n",
    "    return None\n",
    "\n",
    "# Contoh: Mencari kata untuk vektor embedding_matrix[3]\n",
    "word = find_word_by_vector(embedding_matrix, embedding_matrix[345])\n",
    "print(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build and Compile Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size, 8, embeddings_initializer=tf.keras.initializers.Constant(embedding_matrix), \n",
    "              input_shape=(max_length,), trainable=False),\n",
    "    tf.keras.layers.Conv1D(128, 5, activation='relu', padding='same'),\n",
    "    tf.keras.layers.MaxPooling1D(pool_size=2),\n",
    "    tf.keras.layers.LSTM(128, return_sequences=True),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(16, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(6, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looping untuk mendapatkan bobot dari setiap layer\n",
    "for layer in model.layers:\n",
    "    # Mengecek apakah layer memiliki bobot\n",
    "    if layer.weights:\n",
    "        print(\"Layer Name:\", layer.name)\n",
    "        # Mendapatkan bobot dan bias dari layer\n",
    "        weights = layer.get_weights()\n",
    "        for i, w in enumerate(weights):\n",
    "            print(\"Weights Shape (Tensor {}):\".format(i), w.shape)\n",
    "            print(\"Weights (Tensor {}):\".format(i))\n",
    "            print(w)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m202/202\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 191ms/step - accuracy: 0.5691 - loss: 1.1277 - val_accuracy: 0.5878 - val_loss: 1.1405 - learning_rate: 0.0010\n",
      "Epoch 14/200\n",
      "\u001b[1m202/202\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 170ms/step - accuracy: 0.5913 - loss: 1.0694 - val_accuracy: 0.5972 - val_loss: 1.1337 - learning_rate: 0.0010\n",
      "Epoch 15/200\n",
      "\u001b[1m202/202\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 183ms/step - accuracy: 0.6017 - loss: 1.0377 - val_accuracy: 0.6118 - val_loss: 1.1053 - learning_rate: 0.0010\n",
      "Epoch 16/200\n",
      "\u001b[1m202/202\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 152ms/step - accuracy: 0.6230 - loss: 0.9861 - val_accuracy: 0.6164 - val_loss: 1.0987 - learning_rate: 0.0010\n",
      "Epoch 17/200\n",
      "\u001b[1m202/202\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 158ms/step - accuracy: 0.6353 - loss: 0.9584 - val_accuracy: 0.6373 - val_loss: 1.0633 - learning_rate: 0.0010\n",
      "Epoch 18/200\n",
      "\u001b[1m202/202\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 117ms/step - accuracy: 0.6459 - loss: 0.9264 - val_accuracy: 0.6502 - val_loss: 1.0334 - learning_rate: 0.0010\n",
      "Epoch 19/200\n",
      "\u001b[1m202/202\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 96ms/step - accuracy: 0.6672 - loss: 0.8858 - val_accuracy: 0.6453 - val_loss: 1.0811 - learning_rate: 0.0010\n",
      "Epoch 20/200\n",
      "\u001b[1m202/202\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 98ms/step - accuracy: 0.6710 - loss: 0.8580 - val_accuracy: 0.6679 - val_loss: 1.0523 - learning_rate: 0.0010\n",
      "Epoch 21/200\n",
      "\u001b[1m202/202\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 97ms/step - accuracy: 0.6862 - loss: 0.8148 - val_accuracy: 0.6631 - val_loss: 1.0400 - learning_rate: 0.0010\n",
      "Epoch 22/200\n",
      "\u001b[1m202/202\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 95ms/step - accuracy: 0.6918 - loss: 0.8064 - val_accuracy: 0.6770 - val_loss: 1.0810 - learning_rate: 0.0010\n",
      "Epoch 23/200\n",
      "\u001b[1m202/202\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 91ms/step - accuracy: 0.7007 - loss: 0.7566 - val_accuracy: 0.6711 - val_loss: 1.1439 - learning_rate: 0.0010\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "num_epochs = 200\n",
    "\n",
    "# EarlyStopping callback\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "# ReduceLROnPlateau callback\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=10, min_lr=0.0001)\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    train_padded, \n",
    "    y_train, \n",
    "    batch_size=128, \n",
    "    epochs=num_epochs, \n",
    "    validation_data=(valid_padded, y_valid), \n",
    "    callbacks=[early_stopping, reduce_lr]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the training history\n",
    "def plot_graphs(history, string):\n",
    "  plt.plot(history.history[string])\n",
    "  plt.plot(history.history['val_'+string])\n",
    "  plt.xlabel(\"Epochs\")\n",
    "  plt.ylabel(string)\n",
    "  plt.legend([string, 'val_'+string])\n",
    "  plt.show()\n",
    "\n",
    "plot_graphs(history, \"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the training history\n",
    "def plot_graphs(history, string):\n",
    "  plt.plot(history.history[string])\n",
    "  plt.plot(history.history['val_'+string])\n",
    "  plt.xlabel(\"Epochs\")\n",
    "  plt.ylabel(string)\n",
    "  plt.legend([string, 'val_'+string])\n",
    "  plt.show()\n",
    "\n",
    "plot_graphs(history, \"loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sklearn report\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "loss, accuracy = model.evaluate(valid_padded, y_valid)\n",
    "print(\"Loss: \", loss)\n",
    "print(\"Accuracy: \", accuracy)\n",
    "\n",
    "y_pred = model.predict(valid_padded)\n",
    "y_pred = np.argmax(y_pred, axis=1)\n",
    "print(classification_report(y_valid, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "# Membuat confusion matrix\n",
    "cm = confusion_matrix(y_valid, y_pred)\n",
    "\n",
    "# Membuat heatmap dari confusion matrix\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False, annot_kws={\"size\": 16})\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predict another data input from the model\n",
    "def predict_emotion(text):\n",
    "    # Tokenisasi\n",
    "    text = [text]\n",
    "    sequences = tokenizer.texts_to_sequences(text)\n",
    "    padded = pad_sequences(sequences, maxlen=max_length, truncating=trunc_type, padding=\"post\")\n",
    "    \n",
    "    # Prediksi\n",
    "    pred = model.predict(padded)\n",
    "\n",
    "    print(\"Prediksi:\", pred)\n",
    "    pred = np.argmax(pred, axis=1)\n",
    "    \n",
    "\n",
    "    # Mengembalikan label emosi\n",
    "    if pred == 0:\n",
    "        return \"Neutral\"\n",
    "    elif pred == 1:\n",
    "        return \"Anger\"\n",
    "    elif pred == 2:\n",
    "        return \"Joy\"\n",
    "    elif pred == 3:\n",
    "        return \"Love\"\n",
    "    elif pred == 4:\n",
    "        return \"Sad\"\n",
    "    elif pred == 5:\n",
    "        return \"Fear\"\n",
    "\n",
    "#make 10 senteces twitter in predict emotion\n",
    "predict_emotion(\"saya sangat senang hari ini\")\n",
    "predict_emotion(\"saya sangat marah hari ini\")\n",
    "predict_emotion(\"saya sangat cinta hari ini\")\n",
    "predict_emotion(\"saya sangat sedih hari ini\")\n",
    "predict_emotion(\"saya sangat takut hari ini\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save model\n",
    "model.save('./model/model-cnnlstm.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
